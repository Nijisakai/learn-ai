# 基础知识

---

## 计算机概述

### 计算机的发展

#### 第一代：真空管

##### ENICA

ENICA（电子数值积分计算机，Electronic Numerical Integrator And Computer）是世界上第一台通用计算机。它是图灵完全的电子计算机，能够重新编程，解决各种计算问题。

ENIAC为美国陆军的弹道研究实验室（Ballistic Research Laboratory，BRL）所使用，用于计算火炮的外弹道。美国军方要求该实验室每天为陆军炮弹部队提供6张射表以便对导弹的研制进行技术鉴定。事实上每张射表都要计算几百条弹道，而每条弹道的数学模型是一组非常复杂的非线性方程组。这些方程组是没有办法求出准确解的，因此只能用数值方法近似地进行计算。按当时的计算工具，实验室即使雇用200多名计算员加班加点工作也大约需要二个多月的时间才能算完一张射表。

当时任职宾夕法尼亚大学莫尔电机工程学院的约翰・莫奇利（John Mauchly）和他的研究生约翰・埃卡特（John Eckert）提出了使用真空管制造电子计算机的设想。1934年，代号**PX项目**的计算机研制工作在宾夕法尼亚大学的电气工程摩尔商学院秘密展开。但由于ENICA使用真空管制作，使得ENICA计算机十分巨大，重达30吨，占地1500平方英尺，包含超过18000个真空管，在运行时需要消耗140千瓦的功率。同时它也比任何机电计算机快得多，每秒可以计算5000次加法运算。ENIAC可以在30秒内计算出人类计算20小时的导弹轨迹。

ENIAC于1946年完成，已无法用于战争。相反，它的首要任务是执行一系列复杂的计算，用于帮助确定氢弹的可行性。ENIAC用于其他目的之外的目的证明了其通用性。ENIAC继续在弹道研究实验室（BRL）管理下运营，直到1955年被拆解。

##### 冯・诺伊曼结构

启动和改变ENIAC程序的任务非常繁琐。但是假设程序可以以适合与数据一起存储在存储器中的形式表示。然后，计算机可以通过从存储器中读取它们来获得其指令，并且可以通过设置一部分存储器的值来设置或改变程序。

这个被称为存储程序概念的想法通常归功于ENIAC设计师，最著名的是数学家冯・诺依曼（John von Neumann），他是ENIAC项目的顾问。阿兰・图灵（Alan Mathison Turing）几乎同时发明了这个想法。这个想法首次发表于冯・诺依曼在1945年提出的一种新计算机 EDVAC（电子离散可变计算机）的提案。

1946年，冯・诺伊曼和他的同事开始在普林斯顿高等研究院开设一种新的存储程序计算机，称为IAS计算机。IAS计算机虽然直到1952年才完成，但它是所有后续通用计算机的原型。

#### 第二代：晶体管

电子计算机的第一个重大变化是用晶体管替换真空管。与真空管相比，晶体管更小，更便宜并且散热更少，但是可以以与真空管相同的方式用于构造计算机。与需要电线，金属板，玻璃胶囊和真空的真空管不同，晶体管是由硅制成的固态器件。

该晶体管于1947年在贝尔实验室被发明，并在20世纪50年代引发了电子革命。然而，直到20世纪50年代后期，完全晶体管化的计算机才被商业化。随后IBM推出了著名的7000系列计算机。

晶体管的使用定义了第二代计算机。基于所采用的基本硬件技术，将计算机分类为几代已被广泛接受。每一代新产品的特点是处理性能更高，内存容量更大，并且尺寸比前一代更小。

![计算机代](http://pic-learn-ai.oss-cn-beijing.aliyuncs.com/Snipaste_2019-08-21_16-36-01.jpg)

除此之外，第二代计算机在结构上也有了一些其他变化。第二代计算机引入了更复杂的算术和逻辑单元和控制单元，使用高级编程语言，以及与计算机一起提供系统软件。从广义上讲，系统软件提供了加载程序，将数据移动到外围设备和库以执行常见计算的能力，类似Windows和Linux等现代操作系统。

第二代也值得注意的是数字设备公司（DEC）的出现。DEC成立于 1957年，并在那一年交付了第一台计算机PDP-1。

#### 第三代：集成电路

单个独立的晶体管称为分立元件。在20世纪50年代和60年代早期，电子设备主要由分立元件组成——晶体管，电阻器，电容器等。分立元件分别制造，封装在自己的容器中，焊接或连接在一起形成类似绝缘纤维板的电路板，然后安装在计算机、示波器和其他电子设备中。每当一个电子设备需要一个晶体管时，一个含有针头大小的硅片的小管子必须焊接到电路板上。从晶体管到电路板的整个制造过程既昂贵又麻烦。

早期的第二代计算机包含大约10000个晶体管。这个数字增长到数十万，使得制造更新，更强大的机器变得越来越困难。

![第一个集成电路图片](http://pic-learn-ai.oss-cn-beijing.aliyuncs.com/theFirstIC.jpg)

1958年取得了革命性的电子技术并开创了微电子时代的成就：集成电路的发明。它是定义第三代计算机的**集成电路**。集成电路利用了诸如晶体管，电阻器和导体之类的部件可以由诸如硅的半导体制造的事实。它仅仅是固态技术的延伸，用于在一小块硅中制造整个电路，而不是将由单独的硅片制成的分立元件组装到同一电路中。许多晶体管可以在单个硅晶片上同时生产。同样重要的是，这些晶体管可以通过金属化工艺连接以形成电路。

最初，只有少数门或存储单元可以可靠地制造和封装在一起。这些早期的集成电路称为小规模集成电路（Small Scale Integration, SSI）。随着时间的推移，可以在同一芯片上打包越来越多的组件，晶体管的密度的增长如图所示。

![增长线图](http://pic-learn-ai.oss-cn-beijing.aliyuncs.com/Transistor_Count_and_Moore's_Law_-_2011.svg)

这个数字也反映了1965年英特尔联合创始人戈登・摩尔（Gordon Moore）提出的着名的摩尔定律。摩尔观察到可以放在单个芯片上的晶体管数量每年翻倍并预测这种速度会继续到不久的将来。令包括摩尔在内的许多人惊讶的是，这种速度一年又一年，十年后持续不断。在 20世纪70年代，节奏减缓到每18个月翻一番，但此后一直保持这一速度。

##### 摩尔定律

摩尔定律（Moore's law）是由英特尔创始人之一戈登・摩尔（Gordon Moore）提出的。其内容为：集成电路上可容纳的晶体管数目，约每隔两年便会增加一倍。经常被引用的**18个月**，是由英特尔首席执行官大卫・豪斯（David House）提出：预计18个月会将芯片的性能提高一倍（即更多的晶体管使其更快），是一种以倍数增长的观测。

尽管摩尔定律的现象已经被观察到了数十年，摩尔定律仍应该被视为是对现象的观测或对未来的推测，而不是一个物理定律或自然界的规律。从另一角度看，未来的增长率在逻辑上无法保证会跟过去的数据一样，也就是逻辑上无法保证摩尔定律会持续下去。虽然预计摩尔定律将持续到至少2020年，然而2010年国际半导体技术发展路线图的更新增长已经在2013年年底放缓；又比如说英特尔在22纳米跟14纳米的CPU制程上已经放慢了技术更新的脚步。

#### 之后的几代

在第三代之后，关于定义几代计算机的一致意见较少。随着大规模集成（Large Scale Integration, LSI）的引入，可以在单个集成电路芯片上放置1000多个元件。超大规模集成（Very Large Scale Integration, VLSI）每个芯片实现了10000多个组件，而目前的超大规模集成（Ultra Large Scale Integration, ULSI）芯片可以包含超过10亿个组件。

##### 半导体储存器

集成电路技术首次应用于计算机是集成电路芯片中处理器（控制单元和算术和逻辑单元）的构造，但人们也发现这种技术可用于构建储存器。

在20世纪50年代和60年代，大多数计算机存储器由铁磁材料的微小环构成，每个直径约为十六分之一英寸。这些环被悬挂在计算机内部小屏幕上的细线网格上。单向磁化，一个环（称为核心）代表**1**， 另一方向磁化，它代表**0**。 磁芯存储器相当快，读取存储在存储器中的一点只花了百万分之一秒，但它昂贵，笨重，并且使用了破坏性读数——读取核心的简单行为会删除存储在其中的数据。因此，需要安装额外的电路以便在提取数据后立即恢复数据。

然后，在1970年，仙童半导体（Fairchild）产生了第一个半导体存储器。这个芯片大小只有一个内核，可以容纳 256 位数据，访问速度比核心储存器快得多，只需要 700 亿分之一秒的读取时间。

1974年，发生了一个重大事件：半导体存储器的每比特价格降至核心存储器的每比特价格之下。在此之后，存储器成本持续快速下降，伴随着物理存储密度的相应增加。这导致了更小、更快的机器，其内存尺寸更大，更昂贵的机器从几年前开始。内存技术的发展以及接下来要讨论的处理器技术的发展，在不到十年的时间里改变了计算机的性质。虽然笨重，昂贵的计算机仍然是这一领域的一部分，但计算机也已经被用于办公机器和个人计算机的**最终用户**。

##### 微处理器

正如存储器芯片上的元件密度持续上升一样，处理器芯片上的元件密度也在不断增加。随着时间的推移，越来越多的元件被放置在每个芯片上，因此构建单个计算机处理器所需的芯片越来越少。

1971年英特尔开发出4004时取得了突破性进展。4004是第一款在单芯片上集成CPU所有组件的芯片——微处理器诞生了。

1972年英特尔推出了8008处理器。这是第一个8位微处理器，性能几乎是4004的两倍。

1974年英特尔8080的推出。这是第一个通用微处理器，与4004和 8008是为特定应用而设计的不同，8080被设计成通用微型计算机的CPU。与8008一样，8080是一个8位微处理器。然而，8080更快，具有更丰富的指令集，并具有大的寻址能力。

同时16位微处理器的开发开始了。但是直到20世纪70年代末才出现了功能强大的通用16位微处理器，其中之一是8086。微处理器的下一次重大升级是发生在1981年，贝尔实验室和惠普公司都开发了32位单芯片微处理器。英特尔于1985年推出了自己的32位微处理器80386。

### 计算机的操作系统

操作系统是管理计算机硬件的程序，它还为应用程序提供基础，并且充当计算机硬件和计算机用户的中介。令人惊奇的是操作系统完成这些任务的方式多种多样。大型机的操作系统设计的主要目的是为了充分优化硬件的使用率，个人计算机的操作系统是为了能支持从复杂游戏到商业应用的各种事物，手持计算机的操作系统是为了给用户提供一个可以与计算机方便地交互并执行程序的环境。因此，有的操作系统设计是为了方便，有的设计是为了高效，而有的设计目标则是兼而有之。

#### 操作系统做了什么

操作系统是几乎所有计算机系统的一个重要部分。计算机系统可以大致分为4个组成部分：计算机硬件、操作系统、系统程序与应用程序和用户。

硬件，如中央处理单元（Central Processing Unit，CPU）、内存（memory）输入输出设备（input/output devices，I/O devices），为系统提供基本的计算资源。应用程序如字处理程序、电子制表软件、编译器、网络浏览器规定了用户按何种方式使用这些资源。操作系统控制和协调各用户的应用程序对硬件的使用。

计算机系统的组成部分包括硬件、软件及数据。在计算机系统的操作过程中，操作系统提供了正确使用这些资源的方法。操作系统类似于政府。与政府一样，操作系统本身并不能实现任何有用的功能。它只不过提供了一个方便其他程序做有用工作的环境。

为了更加全面地理解操作系统所担当的角色，接下来从两个视角探索操作系统:即从用户的视角和系统的视角来讨论。

##### 用户视角

计算机的用户观点因所使用接口的不同而异。绝大多数计算机用户坐在一台这样的PC前，PC由显示器、键盘、鼠标和主机组成。这类系统设计是为了让单个用户单独使用其资源，其目的是优化用户所进行的工作（或游戏）。对于这种情况，操作系统的设计目的是为了用户使用方便，性能是次要的，而且不在乎资源使用率——如何共享硬件和软件资源。性能对用户来说非常重要，而不是资源使用率，这种系统主要为了优化单用户的情况。

在某些情况下，有些用户坐在与大型机或小型机相连的终端前，其他用户通过其他的终端访问同一计算机。这些用户共享资源并可交换信息。操作系统设计为资源使用做了优化:确保所有的CPU时间、内存和I/O都能得到充分使用，并且确保没有用户使用超出其权限以外的资源。

在另一些情况下，其他用户坐在工作站前，工作站与其他工作站和服务器相连。这些用户不但可以使用专用的资源，而且可以使用共享资源，如网络和服务器及文件、计算和打印服务器。因此，这类操作系统的设计目的是个人使用性能和资源利用率的折中。

近来，各种手持计算机开始成为时尚。绝大多数这些设备为单个用户所独立使用。有的也通过有线或（更为常见）无线与网络相连。由于受电源、速度和接口所限，它们只能执行相对较少的远程操作。绝大多数这类操作系统的设计目的是为了方便个人使用，当然如何在有限的电池容量中发挥最大的效用也很重要。

有的计算机几乎没有或根本没有用户观点。例如，在家电和汽车中所使用的嵌入式计算机可能只有键盘，只能打开和关闭指示灯来显示状态，而且这些设备及其操作系统通常设计成无需用户干预就能自行运行。

##### 系统视角

从计算机的角度来看，操作系统是与硬件最为密切的程序，可以将操作系统看作资源分配器。计算机系统可能有许多资源，用来解决CPU时间、内存空间、文件存储空间、I/O设备等问题。操作系统管理这些资源。面对许多甚至冲突的资源请求，操作系统必须决定如何为各个程序和用户分配资源以便计算机系统能有效而公平地运行。众所周知，资源分配对多用户访问主机或微型计算机特别重要。

操作系统的一个稍稍不同的观点是强调控制各种设备和用户程序的需要。操作系统是控制程序。控制程序管理用户程序的执行以防止计算机资源的错误使用或使用不当。它特别关注I/O设备的操作和控制。

### 计算机语言与程序设计

编程语言（英语：Programming Language），是用来定义计算机程序的形式语言。它是一种被标准化的交流技巧，用来向计算机发出指令。一种计算机语言让程序员能够准确地定义计算机所需要使用的数据，并精确地定义在不同情况下所应当采取的行动。

最早的编程语言是在计算机发明之前产生的，当时是用来控制提花织布机及自动演奏钢琴的动作。在计算机领域已发明了上千不同的编程语言，而且每年仍有新的编程语言诞生。很多编程语言需要用指令方式说明计算的程序，而有些编程语言则属于宣告式编程，说明需要的结果，而不说明如何计算。

编程语言的描述一般可以分为语法及语义。语法是说明编程语言中，哪些符号或文字的组合方式是正确的，语义则是对于编程的解释。有些语言是用规格文件定义，例如C语言的规格文件也是ISO标准中一部分，而其他语言（像Perl）有一份主要的编程语言实现文件，视为是参考实现。

#### 历史

##### 早期发展

非常早期的计算机，例如巨人计算机（Colossus），在没有存储程序的帮助下，通过修改其电路或设置物理控制组来编程。

稍后，程序可以用机器语言编写，程序员以硬件可以直接执行的数字形式写入每条指令。例如，在两个存储器位置添加值的指令可能包含3个数字：一个选择 “添加” 操作的 “操作码” 和两个存储器位置。这些程序以十进制或二进制形式从穿孔卡，纸带，磁带读取或在计算机前面板上的开关上切换。机器语言后来被称为第一代编程语言。

下一步是开发第二代编程语言或汇编语言，它们仍然与特定计算机的指令集架构密切相关。这些使得程序更具人性化，并使程序员免于繁琐且容易出错的地址计算。

第一代高级编程语言或第三代编程语言是在20世纪50年代编写的。为计算机设计的早期高级编程语言是Plankalkül，由康拉德・楚泽（Konrad Zuse）在1943年至1945年间为德国Z3计算机开发。然而，它直到 1998 年和 2000 年才编写完成。

约翰・莫奇利（John Mauchly）在1949年提出短代码编程语言（Short Code），是有史以来为电子计算机开发的第一批高级语言之一。与机器代码不同，短代码语句以可理解的形式表示数学表达式。但是，程序每次运行时都必须转换为机器代码，这使得该过程比运行等效的机器代码慢得多。

在曼彻斯特大学，艾里克・格伦尼（Alick Glennie）在20世纪50年代初开发了Autocode。它使用编译器自动将语言转换为机器代码。第一个代码和编译器是在 1952 年为曼彻斯特大学的Mark 1计算机开发的，被认为是第一个编译的高级编程语言。

第二个Autocode是由托尼・布鲁克（RA Brooker）于1954 年为Mark 1开发的，被称为Mark 1 Autocode。布鲁克还在20世纪50年代与曼彻斯特大学合作开发了Ferranti Mercury（一种计算机）的自动编码。这种语言的EDSAC计算机版本在1961年被剑桥大学数学实验室的哈特利（David Hartley）设计，被称为EDSAC 2 Autocade。它是从Mercury Autocode的基础上根据新机器的需求修改而来，可以指出其目标代码的优化和源语言当时推进的诊断。

1954 年，FORTRAN语言由布鲁克（John Backus）在IBM发明。它是第一个广泛使用的高级通用编程语言，具有功能实现，而不仅仅是纸上设计。它仍然是高性能计算的流行语言，用于对世界上最快的超级计算机进行基准测试和排名的程序。

另一种早期编程语言是由格蕾丝・赫柏（Grace Hopper）在美国设计的，名为FLOW-MATIC。它是最初是在1955年至1959年期间由雷明顿兰德公司（Remington Rand）为UNIVAC I计算机开发的。赫柏发现商业数据处理客户对数学符号感到不舒服，并且在 1955 年初，她和她的团队编写了英语编程语言规范并实施原型。FLOW-MATIC编译器于1958年初公开上市，并于1959年基本完成。

##### 细化

越来越多的高级语言的使用引入了对低级编程语言或系统编程语言的要求。这些语言在不同程度上提供汇编语言和高级语言之间的便利，它们可用于执行需要直接访问硬件设施但仍提供更高级别控制结构和错误检查的任务。

从20世纪60年代到70年代末期，现在使用的主要语言范式得到了发展：

APL，引入了阵列编程并影响了函数式编程。
ALGOL，完善了结构化程序编程和语言规范学科；《ALGOL 60算法语言修订报告（Revised Report on the Algorithmic Language ALGOL 60）》成为后来编写语言规范的模型。
Lisp，于 1958 年实现，是第一个动态类型的函数式编程语言。
在 20 世纪 60 年代，Simula是第一种支持面向对象编程的语言 ; 在 20 世纪 70 年代中期，Smalltalk采用了第一种 “纯粹的” 面向对象语言。
C是在 1969 年和 1973 年之间开发的，作为Unix操作系统的系统编程语言，并且仍然很受欢迎。
Prolog，设计于 1972 年，是第一个逻辑编程语言。
1978 年，ML语言在Lisp之上构建了一个多态类型系统，开创了静态类型的 函数式编程语言。
这些语言中的每一种都产生了后代，大多数现代编程语言至少在其祖先中至少有一种。

20世纪60年代和70年代也对结构化编程的优点进行了大量辩论，以及编程语言是否应该被设计为支持它。艾兹赫尔・戴克斯特拉（Edsger Dijkstra）在1968年出版的《ACM通讯》中发表的着名论文中指出，应该从所有**更高级别**的编程语言中删除GOTO语句。

##### 整合和增长

20世纪80年代是相对稳定的年代。C++结合了面向对象和系统编程。美国政府对Ada语言进行了标准化，Ada是一种源自Pascal的系统编程语言，旨在供国防承包商使用。日本和其他地方花费了大量资金来研究所谓的“第五代” 语言，这些语言包含了逻辑编程结构。所有这些研究与运动都没有发明新范式，而是阐述了前几十年发明的思想。

在20世纪80年代，用于编程大规模系统的语言设计的一个重要趋势是更多地关注模块或大规模组织代码单元的使用。Modula-2、Ada和ML都是在20世纪80年代开发的著名模块系统，它们通常与通用编程结构相结合。

20世纪90年代中期互联网的快速发展为新语言创造了机会。Perl语言最初是1987年首次发布的Unix脚本工具，在动态网站中很常见。Java开始用于服务器端编程，字节码虚拟机在商业环境中再次流行，承诺**一次编写，随处运行**（UCSD Pascal在 20 世纪 80 年代早期流行一段时间）。这些发展并不是根本新颖的，而是对许多现有语言和范例的改进（尽管它们的语法通常基于C系列编程语言）。

在工业和研究领域，编程语言的演变仍在继续。当前的方向包括安全性和可靠性验证，新型模块化和数据库集成，如Microsoft的LINQ。

第四代编程语言是计算机编程语言，旨在提供比第三代更高级别的内部计算机硬件细节抽象。第五代编程语言是基于使用给予程序的约束来解决问题的编程语言，而不是使用程序员编写的算法。

#### 语言分类

##### 机器语言

用机器语言编写程序，编程人员要首先熟记所用计算机的全部指令代码和代码的涵义。手编程序时，程序员得自己处理每条指令和每一数据的存储分配和输入输出，还得记住编程过程中每步所使用的工作单元处在何种状态。这是一件十分繁琐的工作，编写程序花费的时间往往是实际运行时间的几十倍或几百倍。而且，编出的程序全是些 0 和 1 的指令代码。直观性差，还容易出错。除了计算机生产厂家的专业人员外，绝大多数程序员已经不再去学习机器语言了。

##### 汇编语言

为了克服机器语言难读、难编、难记和易出错的缺点，人们就用与代码指令实际含义相近的英文缩写词、字母和数字等符号来取代指令代码（如用 ADD 表示运算符号 “+” 的机器代码），于是就产生了汇编语言。所以说，汇编语言是一种用助记符表示的仍然面向机器的计算机语言。汇编语言亦称符号语言。汇编语言由于是采用了助记符号来编写程序，比用机器语言的二进制代码编程要方便些，在一定程度上简化了编程过程。汇编语言的特点是用符号代替了机器指令代码。而且助记符与指令代码一一对应，基本保留了机器语言的灵活性。使用汇编语言能面向机器并较好地发挥机器的特性，得到质量较高的程序。

汇编语言中由于使用了助记符号，用汇编语言编制的程序送入计算机，计算机不能象用机器语言编写的程序一样直接识别和执行，必须通过预先放入计算机的 “汇编程序 “的加工和翻译，才能变成能够被计算机识别和处理的二进制代码程序。用汇编语言等非机器语言书写好的符号程序称源程序，运行时汇编程序要将源程序翻译成目标程序。目标程序是机器语言程序，它一经被安置在内存的预定位置上，就能被计算机的 CPU 处理和执行。

汇编语言像机器指令一样，是硬件操作的控制信息，因而仍然是面向机器的语言，使用起来还是比较繁琐费时，通用性也差。汇编语言是低级语言。但是，汇编语言用来编制系统软件和过程控制软件，其目标程序占用内存空间少，运行速度快，有着高级语言不可替代的用途。

##### 高级语言

不论是机器语言还是汇编语言都是面向硬件的具体操作的，语言对机器的过分依赖，要求使用者必须对硬件结构及其工作原理都十分熟悉，这对非计算机专业人员是难以做到的，对于计算机的推广应用是不利的。计算机事业的发展，促使人们去寻求一些与人类自然语言相接近且能为计算机所接受的语意确定、规则明确、自然直观和通用易学的计算机语言。这种与自然语言相近并为计算机所接受和执行的计算机语言称高级语言。高级语言是面向用户的语言。无论何种机型的计算机，只要配备上相应的高级语言的编译或解释程序，则用该高级语言编写的程序就可以通用。

如今被广泛使用的高级语言有Java、Python、C、JavaScript等。这些语言都是属于系统软件。

计算机并不能直接地接受和执行用高级语言编写的源程序，源程序在输入计算机时，通过 “翻译程序” 翻译成机器语言形式的目标程序，计算机才能识别和执行。这种 “翻译” 通常有两种方式，即编译方式和解释方式。编译方式是：事先编好一个称为编译程序的机器语言程序，作为系统软件存放在计算机内，当用户由高级语言编写的源程序输入计算机后，编译程序便把源程序整个地翻译成用机器语言表示的与之等价的目标程序，然后计算机再执行该目标程序，以完成源程序要处理的运算并取得结果。解释方式是：源程序进入计算机时，解释程序边扫描边解释作逐句输入逐句翻译，计算机一句句执行，并不产生目标程序。PASCAL、 FORTRAN、COBOL 等高级语言执行编译方式； BASIC 语言则以执行解释方式为主；而 PASCAL、C 语言是能书写编译程序的高级程序设计语言。每一种高级（程序设计）语言，都有自己人为规定的专用符号、英文单词、语法规则和语句结构（书写格式）。高级语言与自然语言（英语）更接近，而与硬件功能相分离（彻底脱离了具体的指令系统），便于广大用户掌握和使用。高级语言的通用性强，兼容性好，便于移植。

## 人工智能概述

人工智能是计算机科学的一个分支，她企图了解智能的实质，并产生一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统。

### 人工智能的发展

谈到人工智能，就不能不提到鼻祖式人物：图灵。1936年，英国数学家、逻辑学家阿兰・麦席森・图灵（1912~1954）提出了一种抽象的计算模型——图灵机(Turing Machine），用纸带式机器来模拟人们进行数学运算的过程，图灵本人被视为计算机科学之父。

1959年，图灵发表了一篇划时代的论文《计算机器与智能》，文中提出了人工智能领域著名的图灵测试——如果电脑能在5分钟内回答由人类测试者提出的一系列问题，且其超过30%的回答让测试者误认为是人类所答，则电脑就通过测试并可下结论为机器具有智能。

图灵测试的概念极大影响人工智能对于功能的定义，在这个途径上，卡内基・梅隆大学两位科学家纽厄尔（A.Newell）和西蒙（H.Simon）的“逻辑理论家”程序非常精妙地证明了罗素《数学原理》52道中的38道。西蒙宣称在10年之内，机器就可以达到和人类智能一样的高度。

第一批人工智能探索者找到共同的语言后，于整整60年前的1956年，在美国达特茅斯大学开了一次会，希望确立人工智能作为一门科学的任务和完整路径。与会者们也宣称，人工智能的特征都可以被精准描述，精准描述后就可以用机器来模拟和实现。后来普遍认为，达特茅斯会议标志着人工智能的正式诞生。

达特茅斯会议推动了全球第一次人工智能浪潮的出现，即为1956年到1974年。当时乐观的气氛弥漫着整个学界，在算法方面出现了很多世界级的发明，其中包括一种叫做增强学习的雏形（即贝尔曼公式）现在常听到的深度学习模型，其雏形叫做感知器，也是在那几年间发明的。除了算法和方法论有了新的进展，在第一次浪潮中，科学家们还造出了聪明的机器。

#### 人工智能第一次浪潮和寒冬

在80年代出现了人工智能数学模型方面的重大发明，其中包括著名的多层神经网络（1986）和BP反向传播算法（1986）等，也出现了能与人类下象棋的高度智能机器（1989）。

但在1974至1980年人工智能进入冬天，因为人们发现逻辑证明器、感知器、增强学习等等只能做很简单、非常专门且很窄的任务，稍微超出范围就无法应对。这里面存在两方面局限：一方面，人工智能所基于的数学模型和数学手段被发现有一定的缺陷；另一方面，有很多计算复杂度以指数程度增加，所以成为了不可能完成的计算任务。

#### 现代PC“促成”第二次人工智能寒冬

然而，1987年到1993年现代PC的出现，让人工智能的寒冬再次降临。相比于现代PC，专家系统被认为古老陈旧而非常难以维护。于是，政府经费开始下降，寒冬又一次来临。

#### 现代AI 的发展

现代AI的曙光发生在这个阶段，出现了新的数学工具、新的理论和摩尔定律。人工智能也在确定自己的方向，其中一个选择就是要做实用性、功能性的人工智能，这导致了一个新的人工智能路径。由于对于人工智能任务的明确和简化，带来了新的繁荣。

在新的数学工具方面，原来已经存在于数学或者其他学科的文献中的数学模型，被重新发掘或者发明出来。当时比较显著几个成果包括最近获得图灵奖的图模型以及图优化、深度学习网络等，都是大约在15年前重新被提出来，重新开始研究。

在新的理论方面，由于数学模型对自然世界的简化，有着非常明确的数理逻辑，使得理论分析和证明成为可能，可以分析出到底需要多少数据量和计算量来以得期望的结果，这对开发相应的计算系统非常有帮助。

在更重要的一方面，摩尔定律让计算越来越强大，而强大计算机很少被用在人工智能早期研究中，因为早期的人工智能研究更多被定义为数学和算法研究。当更强大的计算能力被转移到人工智能研究后，显著提高了人工智能的研究效果。

由于这一系列的突破，人工智能又产生了一个新的繁荣期。

#### 人工智能商业化浪潮

2016年IBM在全球范围内倾全力推出的**认知商业**，才是真正意义上的人工智能商业化第一波浪潮。在**深蓝**成功后，IBM研究院进而挑战人工智能的深度问答，这是人工智能的一个重要分支，具有极为广阔的应用空间。

### 人工智能的关键技术

#### 人机交互

关于人机交互，它最重要的方面研究人和计算机之间的信息交换，主要包括人到计算机和计算机到人的两部分信息交换，是人工智能领域的重要的外围技术。人机交互是与认知心理学、人机工程学、多媒体技术、虚拟现实技术等密切相关的综合学科。传统的人与计算机之间的信息交换主要依靠交互设备进行，主要包括键盘、鼠标、操纵杆、数据服装、眼动跟踪器、位置跟踪器、数据手套、压力笔等输入设备，以及打印机、绘图仪、显示器、头盔式显示器、音箱等输出设备。人机交互技术除了传统的基本交互和图形交互外，还包括语音交互、情感交互、体感交互及脑机交互等技术。

#### 云计算与大数据

关于大数据和云计算的关系人们通常会有误解。而且也会把他们混起来说，分别做一句话解释就是：云计算就是硬件资源的虚拟化；大数据就是海量数据的高效处理。另外，如果做一个更形象的解释，云计算相当于我们的计算机和操作系统，将大量的硬件资源虚拟化之后再进行分配使用。

就目前而言，要想发展好大数据，就离不开云计算，我们在进行大数据的时候同样也是离不开云计算的，于是很多人觉得大数据与云计算都有一定的关系，那么大家知道不知道大数据的云计算有什么关系呢？我们在这篇文章中给大家带来这个问题的答案。

首先我们说一下大数据的定义吧，大数据就是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力来适应海量、高增长率和多样化的信息资产。同样也是一种规模大到在获取、存储、管理、分析方面大大超出了传统数据库软件工具能力范围的数据集合，具有海量的数据规模、快速的数据流转、多样的数据类型和价值密度低四大特征。

那么大数据的技术有什么意义呢？大数据的意义并不是在于掌握庞大的数据信息，而在于对这些含有意义的数据进行专业化处理。换而言之，如果把大数据比作一种产业，那么这种产业实现盈利的关键，在于提高对数据的优化能力，通过优化实现数据的增值。

而大数据与云计算的关系在技术上的联系也是密不可分。大数据必然无法用单台的计算机进行处理，必须采用分布式架构。它的特色在于对海量数据进行分布式数据挖掘。但它必须依托云计算的分布式处理、分布式数据库和云存储、虚拟化技术。随着云时代的来临，大数据也吸引了越来越多的关注。分析师团队认为，大数据通常用来形容一个公司创造的大量非结构化数据和半结构化数据，这些数据在下载到关系型数据库用于分析时会花费很多的财力和物力。大数据分析常和云计算联系到一起，因为实时的大型数据集分析需要框架来向数十、数百或甚至数千的电脑分配工作。并且，大数据需要特殊的技术，以有效地处理大量的容忍经过时间内的数据。适用于大数据的技术，包括大规模并行处理数据库、数据挖掘、分布式文件系统、分布式数据库、云计算平台、互联网和可扩展的存储系统。

#### 知识图谱

知识图谱属于AI领域的是一个分支，很多人觉得它和CV（Computer Vision，计算机视觉），ASR（Automatic Speech Recognition，语音识别），以及NLP（Natural Language Processing，自然语言处理）一样都是特指的某一项技术，其实这么理解并不准确，它应该算是多种技术融合后的一种综合型技术。

知识图谱的历史最早要追溯到2012年，由Google公司提出主要用于提升搜索引擎的检索效率，但随着其发展其背后更深刻意义，远不仅是提高检索效率这么简单，而是整个搜索引擎结构的整体转型：将传统基于关键字的搜索模型转向基于语义的搜索升级。

如今针对知识图谱的技术方案已被国内外多家搜索引擎公司所采用，如：美国的微软必应，中国的百度、搜狗等，都在在短短的一年内纷纷宣布了各自的知识图谱产品，足以看出这革新对整个搜索引擎界的整体影响。

但现在这项技术的应用并不仅拘泥于搜索引擎领域范围，很多的数据分析软件，CRM系统（Customer Relationship Management System，客户关系管理系统）也开始采用基于知识图谱的模式去处理数据，从而去深入发现数据更大的价值。

知识图谱从字面上看，可以拆分为知识+图谱，这样我们就可以理解：将需要的知识数据（结构化或非结构化数据）以图谱的形式进行展示，这种简单的过程也是知识图谱的构建过程。

知识图谱为互联网上海量、异构、动态的大数据表达、组织、管理以及利用提供了一种更为有效的方式，使得网络的智能化水平更高，更加接近于人类的认知思维。

目前，知识图谱已在智能搜索、深度问答、社交网络以及一些垂直行业中有所应用，成为支撑这些应用发展的动力源泉。

#### 机器学习

机器学习（Machine Learning）是一门专门研究计算机怎样才能模拟或实现人类的学习行为，以获取新的知识或技能，使之不断改善自身的性能的学科。机器学习虽然发展了几十年，但还是存在很多没有良好解决的问题，例如图像识别、语音识别、自然语言理解、天气预测、内容推荐等等。机器主要通过大量的训练数据进行训练，程序不断地进行自我学习和修正来训练出一个模型，而模型的本质就是一堆参数用成千上万的参数来描述业务特点，从而接近人类的智力。

机器学习是一门多领域交叉学科，涉及统计学、系统辨识、逼近理论、神经网络、优化理论、计算机科学、脑科学等诸多领域。通过研究计算机怎样模拟或实现人类的学习行为， 以获取新的知识或技能。通过知识结构的不断完善与更新来提升机器自身的性能，这属于人工智能的核心领域。基于数据的机器学习是现代智能技术中的重要方法之一，研究从观测数据（样本）出发寻找规律，利用这些规律对未来数据或无法观测的数据进行预测。 AlphaGo就这项技术一个很成功的体现。

根据学习模式将机器学习分类为监督学习、无监督学习和强化学习等。根据学习方法可以将机器学习分为传统机器学习和深度学习。

深度学习是机器学习的一个子集。深度学习的前身是人工神经网络（Artificial Neural Network, ANN），它的基本特点就是模仿人脑神经元传递和处理信息的模式。

有监督学习：输入的训练数据有特征、有标记，在学习中就是找到特征与标记之间的映射关系，通过标记不断纠正学习中的偏差，使预测率不断提高。这种训练数据有标记的学习称为有监督学习。

无监督学习：让计算机自己去学习怎样做一些事情，所有训练数据没有标记，只有特征。无监督学习有两种思路：第一种，训练时不为其指定明确分类但数据会呈现聚群的结构，彼此相似的类型会聚集在一起。计算机把这些没有标记的数据分成一个个组合，就是聚类；第二种，在成功时采用某种激励制度，即强化学习.

半监督学习：训练数据中有一部分有标记有一部分无标记，没有标记的数量远远大于有标记的数量（这也符合现实）。它的基本规律是：数据的分布必然不完全随机，通过结合有标记的局部特征，以及大量没标记的数据的整体分布，可以得到比较好的分类结果。

#### 自然语言处理

自然语言处理（Natural Language Processing, NLP）就是用计算机来处理、理解以及运用人类语言(如中文、英文等)，它属于人工智能的一个分支，是计算机科学与语言学的交叉学科，又常被称为计算语言学。由于自然语言是人类区别于其他动物的根本标志。没有语言，人类的思维也就无从谈起，所以自然语言处理体现了人工智能的最高任务与境界，也就是说，只有当计算机具备了处理自然语言的能力时，机器才算实现了真正的智能。

从研究内容来看，自然语言处理包括语法分析、语义分析、篇章理解等。从应用角度来看，自然语言处理具有广泛的应用前景。特别是在信息时代，自然语言处理的应用包罗万象，例如：机器翻译、手写体和印刷体字符识别、语音识别及文语转换、信息检索、信息抽取与过滤、文本分类与聚类、舆情分析和观点挖掘等，它涉及与语言处理相关的数据挖掘、机器学习、知识获取、知识工程、人工智能研究和与语言计算相关的语言学研究等。

值得一提的是，自然语言处理的兴起与机器翻译这一具体任务有着密切联系。机器翻译指的是利用计算机自动地将一种自然语言翻译为另外一种自然语言。由于人工进行翻译需要训练有素的双语专家，翻译工作非常耗时耗力。更不用说需要翻译一些专业领域文献时，还需要翻译者了解该领域的基本知识。世界上有超过几千种语言，而仅联合国的工作语言就有六种之多。如果能够通过机器翻译准确地进行语言间的翻译，将大大提高人类沟通和了解的效率。

自然语言处理的困难可以罗列出来很多，不过关键在于消除歧义问题，如词法分析、句法分析、语义分析等过程中存在的歧义问题，简称为消歧。而正确的消歧需要大量的知识，包括语言学知识（如词法、句法、语义、上下文等）和世界知识（与语言无关）。这带来自然语言处理的两个主要困难。首先，语言中充满了大量的歧义，这主要体现在词法、句法及语义三个层次上。歧义的产生是由于自然语言所描述的对象――人类活动非常复杂，而语言的词汇和句法规则又是有限的，这就造成同一种语言形式可能具有多种含义。另外一个方面，消除歧义所需要的知识在获取、表达以及运用上存在困难。由于语言处理的复杂性，合适的语言处理方法和模型难以设计。

目前，人们主要通过两种思路来进行自然语言处理，一种是基于规则的理性主义，另外一种是基于统计的经验主义。理性主义方法认为，人类语言主要是由语言规则来产生和描述的，因此只要能够用适当的形式将人类语言规则表示出来，就能够理解人类语言，并实现语言之间的翻译等各种自然语言处理任务。而经验主义方法则认为，从语言数据中获取语言统计知识，有效建立语言的统计模型。因此只要能够有足够多的用于统计的语言数据，就能够理解人类语言。然而，当面对现实世界充满模糊与不确定性时，这两种方法都面临着各自无法解决的问题。例如，人类语言虽然有一定的规则，但是在真实使用中往往伴随大量的噪音和不规范性。理性主义方法的一大弱点就是鲁棒性差，只要与规则稍有偏离便无法处理。而对于经验主义方法而言，又不能无限地获取语言数据进行统计学习，因此也不能够完美地理解人类语言。二十世纪八十年代以来的趋势就是，基于语言规则的理性主义方法不断受到质疑，大规模语言数据处理成为目前和未来一段时期内自然语言处理的主要研究目标。统计学习方法越来越受到重视，自然语言处理中越来越多地使用机器自动学习的方法来获取语言知识。迈进二十一世纪，我们已经进入了以互联网为主要标志的海量信息时代，这些海量信息大部分是以自然语言表示的。一方面，海量信息也为计算机学习人类语言提供了更多的“素材”，另一方面，这也为自然语言处理提供了更加宽广的应用舞台。例如，作为自然语言处理的重要应用，搜索引擎逐渐成为人们获取信息的重要工具，涌现出以百度、谷歌等为代表的搜索引擎巨头；机器翻译也从实验室走入寻常百姓家，谷歌、百度等公司都提供了基于海量网络数据的机器翻译和辅助翻译工具；基于自然语言处理的中文（输入法如搜狗、微软、谷歌等输入法）成为计算机用户的必备工具；带有语音识别的计算机和手机也正大行其道，协助用户更有效地工作学习。总之，随着互联网的普及和海量信息的涌现，自然语言处理正在人们的日常生活中扮演着越来越重要的作用。

#### 计算机视觉

计算机视觉，英文Computer Vision，简称CV。计算机视觉是一门研究如何使机器“看”的科学，更进一步的说，就是指用摄影机和电脑代替人眼对目标进行识别、跟踪和测量等。

计算机视觉的主要任务就是通过对采集的图片或视频进行处理以获得相应场景的信息。计算机视觉任务的主要类型有以下几种：

##### 物体检测

物体检测是视觉感知的第一步，也是计算机视觉的一个重要分支。物体检测的目标，就是用框去标出物体的位置，并给出物体的类别。

物体检测和图像分类不一样，检测侧重于物体的搜索，而且物体检测的目标必须要有固定的形状和轮廓。图像分类可以是任意的目标，这个目标可能是物体，也可能是一些属性或者场景。

##### 物体识别（狭义）

计算机视觉的经典问题便是判定一组图像数据中是否包含某个特定的物体，图像特征或运动状态。这一问题通常可以通过机器自动解决，但是到目前为止，还没有某个单一的方法能够广泛的对各种情况进行判定：在任意环境中识别任意物体。

现有技术能够也只能够很好地解决特定目标的识别，比如简单几何图形识别、人脸识别、印刷或手写文件识别，或者车辆识别。而且这些识别需要在特定的环境中，具有指定的光照，背景和目标姿态要求。

##### 图像分类

一张图像中是否包含某种物体，对图像进行特征描述是物体分类的主要研究内容。一般说来，物体分类算法通过手工特征或者特征学习方法对整个图像进行全局描述，然后使用分类器判断是否存在某类物体。

图像分类问题就是给输入图像分配标签的任务，这是计算机视觉的核心问题之一。这个过程往往与机器学习和深度学习不可分割。

##### 物体定位

如果说图像识别解决的是what，那么，物体定位解决的则是where的问题。利用计算视觉技术找到图像中某一目标物体在图像中的位置，即定位。

目标物体的定位对于计算机视觉在安防、自动驾驶等领域的应用有着至关重要的意义。

##### 图像分割

在图像处理过程中，有时会需要对图像进行分割来提取有价值的用于后继处理的部分，例如筛选特征点，或者分割一或多幅图片中含有特定目标的部分等。

图像分割指的是将数字图像细分为多个图像子区域（像素的集合，也被称作超像素）的过程。图像分割的目的是简化或改变图像的表示形式，使得图像更容易理解和分析。更精确地说，图像分割是对图像中的每个像素加标签的一个过程，这一过程使得具有相同标签的像素具有某种共同视觉特性。

**图像语意分割**是一个像素级别的物体识别，即每个像素点都要判断它的类别。它和检测的区别是，物体检测是一个物体级别的，他只需要一个框，去框住物体的位置，而通常分割是比检测要更难的问题。

计算机视觉是通过创建人工模型来模拟本由人类执行的视觉任务。其本质是模拟人类的感知与观察的一个过程。这个过程不止识别，而是包含了一系列的过程，并且最终是可以在人工系统中被理解和实现的。

#### 智能机器人

智能机器人之所以叫智能机器人，这是因为它有相当发达的“大脑”。在脑中起作用的是中央处理器，这种计算机跟操作它的人有直接的联系。最主要的是，这样的计算机可以进行按目的安排的动作。正因为这样，我们才说这种机器人才是真正的机器人，尽管它们的外表可能有所不同。

#### 类脑智能

类脑智能又称为类脑计算，上世纪80年代末，美国科学家卡沃・米德（Carver Mead）首次提出类脑计算的概念。类脑计算这一想法摆脱了传统的计算模式，模仿人类神经系统的工作原理，渴求开发出快速、可靠、低耗的运算技术。类脑智能是人工智能的终极目标，但研究类脑智能不可能复制人的大脑。类脑智能希望通过研究人类大脑的工作机理并模拟出一个和人类一样具有思考、学习能力的机器人。类脑智能技术充分学习人脑的思维模式，从仿生角度努力寻求人工智能的突破。这一热门学科前景诱人，应用范围广阔。科学家们曾预言一个国家类脑智能的发展水平将极大程度影响该国在军事、工业等众多行业的发展，因此类脑智能技术的发展显得尤为重要与急迫。

### 人工智能的应用

人工智能应用（Applications of Artificial Intelligence）的泛围很广，包括：医药，诊断，金融贸易，机器人控制，法律，科学发现和玩具。许多千种人工智能应用深入于每种工业的基础。90年代和21世纪初，人工智能技术变成大系统的元素；但很少人认为这属于人工智能领域的成就。

#### 人工智能在金融领域的应用

银行用人工智能系统组织运作，金融投资和管理财产。2001年8月在模拟金融贸易竞赛中机器人战胜了人。

金融机构已长久用人工神经网络系统去发觉变化或规范外的要求，银行使用协助顾客服务系统；帮助核对帐目，发行信用卡和恢复密码等

#### 人工智能在医疗领域的应用

医学临床可用人工智能系统组织病床计划；并提供医学信息。

人工神经网络用来做临床诊断决策支持系统。用人工智能在医学方面还有下列潜在可能：计算机帮助解析医学图像。这样系统帮助扫描数据图像，从计算X光断层图发现疾病，典型应用是发现肿块。

心脏声音分析。

#### 人工智能在家居领域的应用

人工智能在智能家居场景中，一方面将进一步推动家居生活产品的智能化，包括照明系统、影音系统、能源管理系统、安防系统等，实现家居产品从感知到认知到决策的发展，另一方面在于智能家居系统的建立，搭载人工智能的多款产品都有望成为智能家居的核心，包括机器人、智能音箱、智能电视等产品，智能家居系统将逐步实现家居自我学习与控制，从而提供针对不同用户的个性化服务。

#### 人工智能在汽车领域的应用

基于AI的自动驾驶，自动驾驶车辆所需的处理能力是巨大的，而传统的计算机根本无法胜任这项任务。因为自动驾驶不是遵循一套规则或简单的算法，它涉及到深度学习等，换句话说，就涉及人工智能。越来越多的汽车制造商和初创企业都在开发人工智能应用程序，目前在自动驾驶领域有两家公司处于领先地位：谷歌和特斯拉。

基于AI的云服务，人工智能和云服务也是形影不离，相互作用的。无论如何，汽车都需要通过大量的数据来分析完成相应的任务。人工智能云服务的应用确保了数据的有效性，更充分体现了大数据的潜在价值。

基于AI的汽车保险，保险公司利用人工智能实时进行风险评估。

基于AI的汽车制造，人工智能不仅改变了汽车的功能，也改变了汽车的制造方式。新出现的是与人类并肩作战的智能机器人。例如2018年初，起亚汽车开始与现代合作，为其装配线开发可穿戴工业机器人。背心外骨骼(H-VEX)和无椅外骨骼(H-CEX)可穿戴机器人帮助保护工作人员的膝盖、背部和颈部，同时赋予他们更大的灵活性与更强的力量

基于AI的驾驶员监控，以色列初创企业eyeSight借助先进的TOF摄像机和红外传感器，检测驾驶员的行为，

#### 人工智能在零售领域的应用

人工智能在商业界中已经掀起了一阵风潮，其系统的庞大市场规模越来越受到重视。具体体现为店面优化，供应链优化，营销策略的变化，手势识别等方面

#### 人工智能在教育领域的应用

个性化学习，因材施教：分析内容，构建知识图谱，构建和优化内容模型，建立知识图谱，让用户可以更容易地、更准确地发现适合自己的内容。国外这方面的典型应用是分级阅读平台，推荐给用户适宜的阅读材料，并将阅读与教学联系在一起，文后带有小测验，并生成相关阅读数据报告，老师得以随时掌握学生阅读情况。

自动化辅导与答疑：AI除了应用于个性化学习方案的制定外，还落地在自动化辅导和答疑子领域，这也成为了教师面授外的补充。

智能测评：随着信息化建设、人工智能的发展，大数据、文字识别、语音识别、语义识别，使得规模化的自动批改和个性化反馈走向现实。如何利用人工智能减轻批改压力，实现规模化又个性化的作业反馈，是未来教育的重要攻克点

模拟和游戏化教学平台：寓教于乐也是现代教育理念之一。

教育决策：AI 能做的远远不止大学专业选择的分析决策，AI 帮助决策将越来越多地影响我们生活的方方面面

幼儿早教机器人：早教的未来在移动和智能。儿童机器人的门槛不在技术这块，而在于内容、交互方式。

## 物联网概述

### 物联网的发展

物联网的概念英文术语为**Internet of Things**。物联网是指按照约定的协议，将具有**感知、通信、计算**功能的智能物体、系统、信息资源互联起来，实现对物理世界**泛在感知、可靠传输、智慧处理**的智能服务系统。20世纪90年代有关物联网的研究开始萌芽，此后其概念不断地演进和发展。1999年美国麻省理工学院的Kevin Ashton和他的同事首次提出Internet of Things的概念。同年，中科院启动了传感网的研究，并取得了一些科研成果，建立了一些适用的传感网。2005年，国际电信联盟（ITU）在《The Internet of Things》报告中对物联网概念进行扩展。无所不在的物联网通信时代即将来临。

![1](https://md.hass.live/niji/2019-08-20-7762387-0e9b38a90f7a7cf5.png)

进入2009年，物联网得到了真正的起步。

**2019年1月**，奥巴马就任美国总统后，与工商界代表举行了一次圆桌会议。IBM公司代表提出了**智慧地球**的研究计划，建议新政府投资新一代的智慧型基础设施。当年，美国将新能源和物联网列为振兴经济的两大核心武器；

**2019年8月**，温家宝总理在视察无锡时提出建设“感知中国”中心，物联网被正式列为国家五大新兴战略性产业之一，写入“政府工作报告”，物联网在中国受到了全社会极大的关注；

**2019年9月**，欧盟第七框架下RFID和物联网研究项目簇（European Research Cluster on the Internet of Things）发布了《物联网战略研究路线图》研究报告，定义为基于标准的和可互操作的通信协议且具有自配置能力的动态的全球网络基础架构。
至此，物联网真正的建立了起来，随后的几年物联网开始迈上了新的台阶。我们要学习物联网，必然要从技术层面入手，当下从技术的角度来理解，物联网的发展共分为4个阶段，具体如下图所示：

![2](https://md.hass.live/niji/2019-08-20-7762387-59643b998aa8a6c1.png)

物联网当今在中国的受关注程度是在美国、欧盟及其他各国不可比拟的，中国在物联网理念和应用方面可以说是已经走在了世界的前面。近年来，电子信息技术和人工智能应用的发展促使物联网的内涵和外延有了很大的拓展，物联网已经表现为信息技术和通信技术的发展融合，是信息社会发展的趋势。

### 物联网的关键技术

物联网的关键技术主要包括三个方面：各类终端实现“全面感知”；电信网、互联网等融合实现“可靠创术”；云计算等技术对海量数据的“智能处理”。通过这三个方面的有机结合实现各类资源的“虚拟”和“共享”。

#### 全面感知

全面感知是指利用射频识别（RFID）、传感器、定位器和二维码等手段随时随地对物体进行信息采集和获取。

![3](https://md.hass.live/niji/2019-08-20-7762387-ae563cd9debf05c9.png)

#### 可靠传输

可靠传输是指通过各种电信网络与互联网的融合，对接收到的感知信息进行实时远程传送，实现信息的交互和共享，并进行各种有效的处理。在这一过程中，通常需要用到现有的电信运行网络，包括无线和有线网络。由于传感器网络是一个局部的无线网，因此无线移动通信网、4G、5G网络是作为承载物联网的一个有力的支撑。

![4](https://md.hass.live/niji/2019-08-20-7762387-4a60663d32522c1c.png)

#### 智能处理

物联网是一个智能的网络，面对采集的海量数据，必须通过智能分析和处理才能实现智能化。智能处理是指利用云计算、数据挖掘、模糊识别等各种智能计算技术，对随时接收到的跨地域、跨行业、跨部门的海量数据和信息进行分析处理，提升对物理世界、济社会各种活动和变化的洞察力，实现智能化的决策和控制。

![5](https://md.hass.live/niji/2019-08-20-7762387-486ed4cf97222928.png)

### 边缘计算

边缘计算起源于传媒领域，是指在靠近物或数据源头的一侧，采用网络、计算、存储、应用核心能力为一体的开放平台，就近提供最近端服务。其应用程序在边缘侧发起，产生更快的网络服务响应，满足行业在实时业务、应用智能、安全与隐私保护等方面的基本需求。边缘计算处于物理实体和工业连接之间，或处于物理实体的顶端。而云端计算，仍然可以访问边缘计算的历史数据。对物联网而言，边缘计算技术取得突破，意味着许多控制将通过本地设备实现而无需交由云端，处理过程将在本地边缘计算层完成。这无疑将大大提升处理效率，减轻云端的负荷。由于更加靠近用户，还可为用户提供更快的响应，将需求在边缘端解决。
边缘计算五大优势应对物联网大数据挑战

#### 实时计算，减少反应延迟

边缘计算分布式以及靠近设备端的特性注定它实时处理的优势，所以它能够更好的支撑本地业务实时处理与执行。

#### 可靠性高，离线正常运作

家里的事情就不麻烦远在天边的云计算了，碰到没有网的情况下也一样能运行正常，边缘计算直接对终端设备的数据进行过滤和分析，节能省时效率还高，不受网络限制。

#### 安全合规，满足隐私要求

制造、能源、公共事业等行业要实现智能化，需要整合机械、电子、ICT等跨行业技术，边缘计算首先要实现OT和IT领域的深度协作，并将行业专有技术与知识与ICT数字化技术相结合，满足多种用户需求，无论是什么样子的安全协议，他都将支持。

#### 高性价比，节省存储运输成本

按照IDC的统计数据，到2020年将有超过500亿的终端与设备联网，未来超过50%的数据需要在网络边缘侧分析、处理与储存。边缘计算所面对的市场规模非常巨大，可以存储可运输，不用大型的服务器机房，照样能够运行。

#### 灵活部署，新旧设备互通

不用担心新设备的诞生造成老设备相关接口的不兼容问题，无论你是智慧城市、智慧家居、智慧医院、在线直播，到智能泊车、自动驾驶、无人机、智能制造还是其他，我们都可以灵活部署到每一个应用领域，且保证新旧设备互通互联。

![7762387-e04a6f8cd1eee531](https://md.hass.live/7762387-e04a6f8cd1eee531.jpg)

如果说云计算就像是天上的云，看得见摸不着，像章鱼的大脑，边缘计算就类似于八爪鱼的那些小爪子，一个爪子就是一个小型的机房，靠近具体的实物。把云计算看作是大脑，那么边缘计算就像是大脑输出的神经触角，这些触角连接到各个终端运行各种动作。

![7762387-ed48bdf69767c572](https://md.hass.live/7762387-ed48bdf69767c572.jpeg)

阿里云边缘计算产品Link Edge已经问世。据说通过这款产品，开发者能够轻松将阿里云的边缘计算能力部署在各种智能设备和计算节点上，比如车载中控、工业流水线控制台、路由器等。另外基于生物识别技术的智能云锁利用本地家庭网关的计算能力，可实现无延时体验，断网了还能开锁，避免“被关在自己家门外”的尴尬。云与边缘的协同计算，还能实现场景化联动，一推开门，客厅的灯就自动打开迎接你回家。

![7762387-5d7341bac891ed0f](https://md.hass.live/7762387-5d7341bac891ed0f.jpeg)

### 传感器

在物联网中，传感器主要负责接收物品**讲话**的内容。传感器技术是从自然信源获取信息并对获取的信息进行处理、变换、识别的一门多学科交叉的现代科学与工程技术，它涉及传感器、信息处理和识别的规划设计、开发、制造、测试、应用及评价改进活动等内容。计算机类似于人的大脑，但仅有大脑而没有感知外界信息的“五官”显然是不够的，计算机也需要它们的**五官**——传感器。

传感器也就是我们物联网系统中主要用于采集物理世界中发生的物理事件和数据，包括各类物理量、标识、音频和视频数据等。物联网数据采集涉及的技术有多种，主要包括传感器、RFID、多媒体信息采集、实时定位等。传感器网络组网和协同信息处理技术实现传感器、RFID等数据采集技术所获取数据的短距离传输、自组织组网及多个传感器对数据的协同信息处理过程。物联网传感器解决的就是人类世界和物理世界的数据获取问题，包括以下几点：

![7762387-a3b29599ed8f20d7](https://md.hass.live/7762387-a3b29599ed8f20d7.png)

传感器是一种检测装置，能感受到被检测的信息，并能将检测感受到的信息，按一定规律变换成为电信号或其他所需形式的信息输出，以满足信息的传输、处理、存储、显示、记录和控制等要求，如下图所示，它是实现自动检测和自动控制的首要环节，我们常用的包括以下几种：

![7762387-9562f966116062d2](https://md.hass.live/7762387-9562f966116062d2.png)

### 物联网的应用

物联网作为一种新兴的信息技术正在迅速向各个行业蔓延，从家庭、医疗保健、物流、汽车、零售到工业制造，物联网产品技术将无处不在。例如，时下流行的共享单车，只要拿出手机扫一扫便可打开智能锁骑行，这些智能锁使用的就是物联网技术。

物联网是一个极其庞大的网络，它包罗万象，涉及各行各业，其实质就是让万事万物通过网络连接起来，实现众多超级智能化的应用。业内预测，未来20年内物联网设备接入数量将越过1万亿台，这意味着物联网产业蕴含着巨大的市场机遇。

![7762387-d96f5bb4a4b2d7c4](https://md.hass.live/7762387-d96f5bb4a4b2d7c4.jpg)

目前物联网的应用主要包括：智能工业、智能农业、智能电网、智能家居、智慧校园、智慧医疗、智能交通、智能物流、环境监测等方面。从市场应用来看，智能工业占据物联网市场主要份额最大。物联网技术的应用提高了生产线过程检测、实时参数采集、生产设备监控、材料消耗监测的能力和水平。生产过程的智能监控、智能控制、智能诊断、智能决策、智能维护水平不断提高。钢铁企业应用各种传感器和通信网络，在生产过程中实现对加工产品的宽度、厚度、温度的实时监控，从而提高了产品质量，优化了生产流程。

![7762387-e647bee23b509393](https://md.hass.live/7762387-e647bee23b509393.png)

#### 物联网在家居领域的应用

智能家居是一个居住环境，是以住宅为平台安装有智能家居系统的居住环境，实施智能家居系统的过程称为智能家居集成。
智能家居通过物联网技术将家中的各种设备连接到一起，提供家电控制、照明控制、窗帘控制、电话远程控制、室内外遥控、防盗报警、环境监测、暖通控制、红外转发以及可编程定时控制等多种功能和手段，让用户有更方便的手段来管理家庭设备。

![7762387-b180a28b0cff4be4](https://md.hass.live/7762387-b180a28b0cff4be4.png)

通过触摸屏、无线遥控器、电话、互联网或者语音识别控制家用设备，更可以执行场景操作，使多个设备形成联动；另一方面，智能家居内的各种设备相互间可以通信，不需要用户指挥也能根据不同的状态互动运行，从而给用户带来最大程度的高效、便利、舒适与安全。与普通家居相比，智能家居不仅提供舒适宜人且高品位的家庭生活空间，还将传统家居环境中那些各自单独存在的设备联为一个整体，形成集系统、结构、服务、管理为一体的高效、安全、便利、环保的居住环境，提供全方位的信息交互功能，帮助家庭与外部保持信息交流畅通，优化人们的生活方式，帮助人们有效安排时间，增强家居生活的安全性，甚至为各种能源费用节约资金。

![7762387-e0b1bb853a90adc1](https://md.hass.live/7762387-e0b1bb853a90adc1.png)

#### 物联网在教育领域的应用

物联网再教育中的应用，目前来说分为具体的五个方面，分别是教学质量监控、学生健康状况监测、信息化教学应用、智慧管理、智慧校园等。

##### 教学质量监控

通常是将物联网与现有教学平台集成，开发阅读器接口中间件，对于需要督导的自律性较差的学生，定时佩戴传感器手表、眼镜等记录学生的多重数据，如脑电图、血压、体温等生理信息及眼动、手部轻微移动等运动信息，引入心理学相关测试技术，得出学生的紧张程度、注意力状况、动脑情况等。将传感器获取的实时数据导入现有教学平台，老师根据这些反馈信息对学生进行有效的督促辅导。

##### 学生健康状况监测

通过门式晨检机感知学生的健康信息,自动采集体温指标,当学生体温异常时,可通过短信等通知家长与老师,当学校出现一定数量体温异常案例时,即可通过应急联动机制,将信息传至医疗机构跟踪处理,防止出现集体疫情；而通过为学生配置运动传感器,可以系统感知其运动指标,避免学校只培养“书呆子”。

##### 信息化教学应用

利用物联网建立泛在学习环境。可以利用智能标签识别需要学习的对象，并且根据学生的学习行为记录，调整学习内容。这是对传统课堂和虚拟实验的拓展，在空间上和交互环节上，通过实地考察和实践，增强学生的体验。例如生物课的实践性教学中需要学生识别校园内的各种植物，可以为每类植物粘贴带有二维码的标签，学生在室外寻找到这些植物后，除了可以知道植物的名字，还可以用手机识别二维码从教学平台上获得相关植物的扩展内容。

##### 智慧管理

物联网在教育管理中可以用于人员考勤、图书管理、设备管理等方面。例如带有RFID标签的学生证可以监控学生进出各个教学设施的情况，以及行动路线。又如将RFID用于图书管理，可通过RFID标签可方便地找到图书，并且可以在借阅图书的时候方便地获取图书信息而不用把书一本一本拿出来扫描。将物联网技术用于实验设备管理可以方便地跟踪设备的位置和使用状态，方便管理。

##### 智慧校园

智能化教学环境，控制物联网在校园内还可用于校内交通管理、车辆管理、校园安全、智能建筑、学生生活服务等领域。例如，在教室里安装光线传感器和控制器，根据光线强度和学生的位置，调整教室内的光照度。控制器也可以和投影仪和窗帘导轨等设备整合，根据投影工作状态决定是否关上窗帘，降低灯光亮度。

![7762387-222d39164ccaddc6](https://md.hass.live/7762387-222d39164ccaddc6.png)

## 开源硬件概述

开源硬件指与自由及开放原始码软件相同方式设计的计算机和电子硬件。开源硬件开始考虑对软件以外的领域开源，是开源文化的一部分。

![7762387-629fea6eb29e798a](https://md.hass.live/7762387-629fea6eb29e798a.png)

### 开源硬件的发展

开源硬件（Open Source Hardware），是指与自由及开放源代码相同方式设计的计算机和电子硬件，是开源文化的一部分。开源文化源于20世纪70年代的黑客亚文化。到了90年代，随着Linux受到大众认可、 Netscape浏览器开放源代码等一系列科技事件，开源运动逐渐进入人们的视野。开源运动最早只有开源软件，并基于互联网进行传播。目前，我们日常生活中使用的手机操作系统安卓（Android）、电脑浏览器 Chrome都是属于开源软件。可以说，在我们的日常生活中，开源软件几乎无处不在。

开源软件推崇任何人都可以自由使用、复制、硏究和改动的思想，深刻影响着开源文化的发展。开源硬件也在这种思想下应运而生。1997年，开放源代码促进会（OSI）推出开源硬件认证计划。1998年，大卫·弗里曼（David Freeman）提出开源硬件规范项目。1999年，非营利组织开放设计基金会（ODF）成立，一场开源硬件的运动悄然发生。

开源运动的一个核心是用户可以自行制造产品，无须支付任何费用。它的长足发展成为创客运动兴起的一个重要的技术因素，被誉为创客之父的克里斯安德森(Chris Anderson）在其著作《创客：新工业革命》中，将**在开源社区中分享设计成果、开展合作的文化规范与使用数字桌面工具设计新产品和通过设计传给商业制造服务商或自行制造称**为创客运动的3个变革性共同点。

### 流行的开源硬件

#### Jetson Nano

Jetson Nano是英伟达（NVIDIA）公司提供的一款面向AI的高性能低功率的开发板。具有HDMI、DP、以太网口、USB3.0、GPIO等多个接口。它开启了嵌入式物联网应用程序的新领域。Jetson Nano支持高分辨率传感器，可以并行处理多个传感器，并且可以在每个传感器流上运行多个现代神经网络。它内置了英伟达的硬件加速系统和OpenCV，还支持许多流行的人工智能框架，使开发人员可以很容易地将他们喜欢的模型和框架集成到产品中。Jetson Nano使所有人都能更容易地访问人工智能，并由相同的基础架构和软件提供支持。

![7762387-aafad0e622c1ab2b](https://md.hass.live/7762387-aafad0e622c1ab2b.jpg)

#### 树莓派（Raspberry Pi）

树莓派是一款针对电脑业余爱好者、教师、学生以及小型企业等用户的迷你电脑。树莓派基于Linux系统，并采用ARM架构处理器作为主芯片，也提供了USB与以太网接口。树莓派没有板载存储芯片，仅留有SD卡座，因而运行树莓派需要提供SD卡.树莓派尤其适合于需要支持用户界面的场合，因为它拥有HDMI输出。HDMI接口意味着我们可以将树莓派直接接入到电视或其他显示屏上，从而以低成本构建web浏览设备来支持与用户的交互。换句话说，树莓派可以看成一台功能相对完备的电脑，尽管性能不高。

![7762387-7dc17f4f2c270622](https://md.hass.live/7762387-7dc17f4f2c270622.jpg)

#### Arduino

Arduino是一款便捷灵活、方便上手的开源电子原型台，包含硬件（各种型号的Arduino板）和软件（Arduino IDE）两部分，由一个欧洲开发团队于2005年冬季开发。Arduino使用Atmel公司的一款微处理器作为主芯片，具有体积小、价格实惠等特性。不仅如此，Arduino在设计之初就考虑到了与不同的外设进行交互，在与现有的电子元件例如传感器或者其他控制器件、LED、步进马达等连接时，几乎不需要增加支持电路。当然，Arduino也可以独立运行，并与软件进行交互。同时，Arduino IDE基于processing IDE开发，灵活且简单。开发语言**Arduino语言**基于wiring语言开发，是对avr-gcc库的二次封装，不要求开发者有太多的编程基础，可以说Arduino对初学者非常友好。

![7762387-86ad0722948230d4](https://md.hass.live/7762387-86ad0722948230d4.png)

#### ESPRESSIF

ESP8266EX是一个价格低廉的开发板，包含WiFi模块和GPIO，可以连接传感器、舵机、马达等各种设备。使用Arduino IDE进行开发编程。可通过网络、串口和蓝牙等多种方式进行通信。该芯片可工作于三种种模式下，分别是：AP模式，station模式以及混合模式，通过常用的AT指令进行控制。其具有以下四个主要特性：

##### 性能稳定

ESP8266EX的工作温度范围大，且能够保持稳定的性能，能适应各种操作环境。

##### 高度集成

ESP8266EX集成了32位Tensilica处理器、标准数字外设接口、天线开关、射频、功率放大器、低噪放大器、过滤器和电源管理模块等，仅需很少的外围电路，可将所占PCB空间降低。

##### 低功耗

ESP8266EX专为移动设备、可穿戴电子产品和物联网应用而设计，通过多项专有技术实现了超低功耗。ESP8266EX具有的省电模式适用于各种低功耗应用场景。

##### 32位Tensilica处理器

ESP8266EX内置超低功耗Tensilica L106 32位RISC处理器，CPU时钟速度最高可达160MHz，支持实时操作系统 （RTOS）和WiFi协议栈，可将高达80%的处理能力留给应用编程和开发。

![7762387-a467983db35cc2b6](https://md.hass.live/7762387-a467983db35cc2b6.jpg)

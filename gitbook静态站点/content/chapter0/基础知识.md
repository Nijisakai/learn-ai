# 基础知识

---

## 计算机概述

### 计算机的发展

#### 第一代：真空管

##### ENICA

ENICA（电子数值积分计算机，Electronic Numerical Integrator And Computer）是世界上第一台通用计算机。它是图灵完全的电子计算机，能够重新编程，解决各种计算问题。

ENIAC为美国陆军的弹道研究实验室（Ballistic Research Laboratory，BRL）所使用，用于计算火炮的外弹道。美国军方要求该实验室每天为陆军炮弹部队提供6张射表以便对导弹的研制进行技术鉴定。事实上每张射表都要计算几百条弹道，而每条弹道的数学模型是一组非常复杂的非线性方程组。这些方程组是没有办法求出准确解的，因此只能用数值方法近似地进行计算。按当时的计算工具，实验室即使雇用200多名计算员加班加点工作也大约需要二个多月的时间才能算完一张射表。

当时任职宾夕法尼亚大学莫尔电机工程学院的中文名（John Mauchly）和他的研究生中文名（John Eckert）提出了使用真空管制造电子计算机的设想。1934年，代号**PX项目**的计算机研制工作在宾夕法尼亚大学的电气工程摩尔商学院秘密展开。但由于ENICA使用真空管制作，使得ENICA计算机十分巨大，重达30吨，占地1500平方英尺，包含超过18000个真空管，在运行时需要消耗140千瓦的功率。同时它也比任何机电计算机快得多，每秒可以计算5000次加法运算。ENIAC可以在30秒内计算出人类计算20小时的导弹轨迹。

ENIAC于1946年完成，已无法用于战争。相反，它的首要任务是执行一系列复杂的计算，用于帮助确定氢弹的可行性。ENIAC用于其他目的之外的目的证明了其通用性。ENIAC继续在用中文全名（BRL）管理下运营，直到1955年被拆解。

##### 冯・诺伊曼结构

启动和改变ENIAC程序的任务非常繁琐。但是假设程序可以以适合与数据一起存储在存储器中的形式表示。然后，计算机可以通过从存储器中读取它们来获得其指令，并且可以通过设置一部分存储器的值来设置或改变程序。

这个被称为存储程序概念的想法通常归功于ENIAC设计师，最著名的是数学家冯・诺依曼（John von Neumann），他是ENIAC项目的顾问。阿兰・图灵（英文名）几乎同时发明了这个想法。这个想法首次发表于冯・诺依曼在1945年提出的一种新计算机 EDVAC（电子离散可变计算机）的提案。

1946年，冯・诺伊曼和他的同事开始在普林斯顿高等研究院开设一种新的存储程序计算机，称为（全名）IAS计算机。IAS计算机虽然直到1952年才完成，但它是所有后续通用计算机的原型。

#### 第二代：晶体管

电子计算机的第一个重大变化是用晶体管替换真空管。与真空管相比，晶体管更小，更便宜并且散热更少，但是可以以与真空管相同的方式用于构造计算机。与需要电线，金属板，玻璃胶囊和真空的真空管不同，晶体管是由硅制成的固态器件。

该晶体管于1947年在贝尔实验室被发明，并在20世纪50年代引发了电子革命。然而，直到20世纪50年代后期，完全晶体管化的计算机才被商业化。随后IBM推出了著名的7000系列计算机。

晶体管的使用定义了第二代计算机。基于所采用的基本硬件技术，将计算机分类为几代已被广泛接受。每一代新产品的特点是处理性能更高，内存容量更大，并且尺寸比前一代更小。

![计算机代](http://pic-learn-ai.oss-cn-beijing.aliyuncs.com/Snipaste_2019-08-21_16-36-01.jpg)

除此之外，第二代计算机在结构上也有了一些其他变化。第二代计算机引入了更复杂的算术和逻辑单元和控制单元，使用高级编程语言，以及与计算机一起提供系统软件。从广义上讲，系统软件提供了加载程序，将数据移动到外围设备和库以执行常见计算的能力，类似Windows和Linux等现代操作系统。

第二代也值得注意的是数字设备公司（DEC）的出现。DEC成立于 1957年，并在那一年交付了第一台计算机PDP-1。

#### 第三代：集成电路

单个独立的晶体管称为分立元件。在20世纪50年代和60年代早期，电子设备主要由分立元件组成——晶体管，电阻器，电容器等。分立元件分别制造，封装在自己的容器中，焊接或连接在一起形成类似绝缘纤维板的电路板，然后安装在计算机、示波器和其他电子设备中。每当一个电子设备需要一个晶体管时，一个含有针头大小的硅片的小管子必须焊接到电路板上。从晶体管到电路板的整个制造过程既昂贵又麻烦。

早期的第二代计算机包含大约10000个晶体管。这个数字增长到数十万，使得制造更新，更强大的机器变得越来越困难。

![第一个集成电路图片](http://pic-learn-ai.oss-cn-beijing.aliyuncs.com/theFirstIC.jpg)

1958年取得了革命性的电子技术并开创了微电子时代的成就：集成电路的发明。它是定义第三代计算机的**集成电路**。集成电路利用了诸如晶体管，电阻器和导体之类的部件可以由诸如硅的半导体制造的事实。它仅仅是固态技术的延伸，用于在一小块硅中制造整个电路，而不是将由单独的硅片制成的分立元件组装到同一电路中。许多晶体管可以在单个硅晶片上同时生产。同样重要的是，这些晶体管可以通过金属化工艺连接以形成电路。

最初，只有少数门或存储单元可以可靠地制造和封装在一起。这些早期的集成电路称为小规模集成电路（small-scale integration，SSI）。随着时间的推移，可以在同一芯片上打包越来越多的组件，晶体管的密度的增长如图所示。

![增长线图](http://pic-learn-ai.oss-cn-beijing.aliyuncs.com/Transistor_Count_and_Moore's_Law_-_2011.svg)

这个数字也反映了1965年英特尔联合创始人戈登・摩尔（Gordon Moore）提出的着名的摩尔定律。摩尔观察到可以放在单个芯片上的晶体管数量每年翻倍并预测这种速度会继续到不久的将来。令包括摩尔在内的许多人惊讶的是，这种速度一年又一年，十年后持续不断。在 20世纪70年代，节奏减缓到每18个月翻一番，但此后一直保持这一速度。

##### 摩尔定律

摩尔定律（Moore's law）是由英特尔创始人之一戈登・摩尔（Gordon Moore）提出的。其内容为：集成电路上可容纳的晶体管数目，约每隔两年便会增加一倍。经常被引用的**18个月**，是由英特尔首席执行官大卫・豪斯（David House）提出：预计18个月会将芯片的性能提高一倍（即更多的晶体管使其更快），是一种以倍数增长的观测。

尽管摩尔定律的现象已经被观察到了数十年，摩尔定律仍应该被视为是对现象的观测或对未来的推测，而不是一个物理定律或自然界的规律。从另一角度看，未来的增长率在逻辑上无法保证会跟过去的数据一样，也就是逻辑上无法保证摩尔定律会持续下去。虽然预计摩尔定律将持续到至少2020年，然而2010年国际半导体技术发展路线图的更新增长已经在2013年年底放缓；又比如说英特尔在22纳米跟14纳米的CPU制程上已经放慢了技术更新的脚步。

#### 之后的几代

在第三代之后，关于定义几代计算机的一致意见较少。随着大规模集成（LSI）的引入，可以在单个集成电路芯片上放置1000多个元件。超大规模集成（VLSI）每个芯片实现了10000多个组件，而目前的超大规模集成（ULSI）芯片可以包含超过10亿个组件。

##### 半导体储存器

集成电路技术首次应用于计算机是集成电路芯片中处理器（控制单元和算术和逻辑单元）的构造，但人们也发现这种技术可用于构建储存器。

在20世纪50年代和60年代，大多数计算机存储器由铁磁材料的微小环构成，每个直径约为十六分之一英寸。这些环被悬挂在计算机内部小屏幕上的细线网格上。单向磁化，一个环（称为核心）代表**1**， 另一方面磁化，它代表**0**。 磁芯存储器相当快，读取存储在存储器中的一点只花了百万分之一秒，但它昂贵，笨重，并且使用了破坏性读数——读取核心的简单行为会删除存储在其中的数据。因此，需要安装额外的电路以便在提取数据后立即恢复数据。

然后，在1970年，（中文名）Fairchild产生了第一个半导体存储器。这个芯片大小只有一个内核，可以容纳 256 位数据，访问速度比核心储存器快得多，只需要 700 亿分之一秒的读取时间。

1974年，发生了一个重大事件：半导体存储器的每比特价格降至核心存储器的每比特价格之下。在此之后，存储器成本持续快速下降，伴随着物理存储密度的相应增加。这导致了更小、更快的机器，其内存尺寸更大，更昂贵的机器从几年前开始。内存技术的发展以及接下来要讨论的处理器技术的发展，在不到十年的时间里改变了计算机的性质。虽然笨重，昂贵的计算机仍然是这一领域的一部分，但计算机也已经被用于办公机器和个人计算机的**最终用户**。

##### 微处理器

正如存储器芯片上的元件密度持续上升一样，处理器芯片上的元件密度也在不断增加。随着时间的推移，越来越多的元件被放置在每个芯片上，因此构建单个计算机处理器所需的芯片越来越少。

1971年英特尔开发出4004时取得了突破性进展。4004是第一款在单芯片上集成CPU所有组件的芯片——微处理器诞生了。

1972年英特尔推出了8008处理器。这是第一个8位微处理器，性能几乎是4004的两倍。

1974年英特尔8080的推出。这是第一个通用微处理器，与4004和 8008是为特定应用而设计的不同，8080被设计成通用微型计算机的 CPU。与8008一样，8080是一个8位微处理器。然而，8080更快，具有更丰富的指令集，并具有大的寻址能力。

同时16位微处理器的开发开始了。但是直到20世纪70年代末才出现了功能强大的通用16位微处理器，其中之一是8086。微处理器的下一次重大升级是发生在1981年，贝尔实验室和惠普公司都开发了32位单芯片微处理器。英特尔于1985年推出了自己的32位微处理器80386。

### 计算机的操作系统

### 计算机语言与程序设计

## 人工智能概述

人工智能是计算机科学的一个分支，她企图了解智能的实质，并产生一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统。

### 人工智能的发展

谈到人工智能，就不能不提到鼻祖式人物：图灵。1936年，英国数学家、逻辑学家阿兰·麦席森·图灵(1912~1954)提出了一种抽象的计算模型——图灵机(TuringMachine），用纸带式机器来模拟人们进行数学运算的过程，图灵本人被视为计算机科学之父。

1959年，图灵发表了一篇划时代的论文《计算机器与智能》，文中提出了人工智能领域著名的图灵测试——如果电脑能在5分钟内回答由人类测试者提出的一系列问题，且其超过30%的回答让测试者误认为是人类所答，则电脑就通过测试并可下结论为机器具有智能。

图灵测试的概念极大影响人工智能对于功能的定义，在这个途径上，卡耐基·梅隆两位科学家A.Newell和H.Simon的“逻辑理论家”程序非常精妙地证明了罗素《数学原理》52道中的38道。Simon宣称在10年之内，机器就可以达到和人类智能一样的高度。

第一批人工智能探索者找到共同的语言后，于整整60年前的1956年，在美国达特茅斯大学开了一次会，希望确立人工智能作为一门科学的任务和完整路径。与会者们也宣称，人工智能的特征都可以被精准描述，精准描述后就可以用机器来模拟和实现。后来普遍认为，达特茅斯会议标志着人工智能的正式诞生。
达特茅斯会议推动了全球第一次人工智能浪潮的出现，即为1956年到1974年。当时乐观的气氛弥漫着整个学界，在算法方面出现了很多世界级的发明，其中包括一种叫做增强学习的雏形（即贝尔曼公式）现在常听到的深度学习模型，其雏形叫做感知器，也是在那几年间发明的。除了算法和方法论有了新的进展，在第一次浪潮中，科学家们还造出了聪明的机器。

#### 人工智能第一次浪潮和寒冬

在80年代出现了人工智能数学模型方面的重大发明，其中包括著名的多层神经网络（1986）和BP反向传播算法（1986）等，也出现了能与人类下象棋的高度智能机器（1989）。

但在1974至1980年人工智能进入冬天，因为人们发现逻辑证明器、感知器、增强学习等等只能做很简单、非常专门且很窄的任务，稍微超出范围就无法应对。这里面存在两方面局限：一方面，人工智能所基于的数学模型和数学手段被发现有一定的缺陷；另一方面，有很多计算复杂度以指数程度增加，所以成为了不可能完成的计算任务。

#### 现代PC“促成”第二次人工智能寒冬

然而，1987年到1993年现代PC的出现，让人工智能的寒冬再次降临。相比于现代PC，专家系统被认为古老陈旧而非常难以维护。于是，政府经费开始下降，寒冬又一次来临。

#### 现代AI 的发展

现代AI的曙光发生在这个阶段，出现了新的数学工具、新的理论和摩尔定律。人工智能也在确定自己的方向，其中一个选择就是要做实用性、功能性的人工智能，这导致了一个新的人工智能路径。由于对于人工智能任务的明确和简化，带来了新的繁荣。

在新的数学工具方面，原来已经存在于数学或者其他学科的文献中的数学模型，被重新发掘或者发明出来。当时比较显著几个成果包括最近获得图灵奖的图模型以及图优化、深度学习网络等，都是大约在15年前重新被提出来，重新开始研究。

在新的理论方面，由于数学模型对自然世界的简化，有着非常明确的数理逻辑，使得理论分析和证明成为可能，可以分析出到底需要多少数据量和计算量来以得期望的结果，这对开发相应的计算系统非常有帮助。

在更重要的一方面，摩尔定律让计算越来越强大，而强大计算机很少被用在人工智能早期研究中，因为早期的人工智能研究更多被定义为数学和算法研究。当更强大的计算能力被转移到人工智能研究后，显著提高了人工智能的研究效果。

由于这一系列的突破，人工智能又产生了一个新的繁荣期。

#### 人工智能商业化浪潮

2016年IBM在全球范围内倾全力推出的**认知商业**，才是真正意义上的人工智能商业化第一波浪潮。在**深蓝**成功后，IBM研究院进而挑战人工智能的深度问答，这是人工智能的一个重要分支，具有极为广阔的应用空间。

### 人工智能的关键技术

#### 人机交互

关于人机交互，它最重要的方面研究人和计算机之间的信息交换，主要包括人到计算机和计算机到人的两部分信息交换，是人工智能领域的重要的外围技术。人机交互是与认知心理学、人机工程学、多媒体技术、虚拟现实技术等密切相关的综合学科。传统的人与计算机之间的信息交换主要依靠交互设备进行，主要包括键盘、鼠标、操纵杆、数据服装、眼动跟踪器、位置跟踪器、数据手套、压力笔等输入设备，以及打印机、绘图仪、显示器、头盔式显示器、音箱等输出设备。人机交互技术除了传统的基本交互和图形交互外，还包括语音交互、情感交互、体感交互及脑机交互等技术。

#### 云计算与大数据

关于大数据和云计算的关系人们通常会有误解。而且也会把他们混起来说，分别做一句话解释就是：云计算就是硬件资源的虚拟化；大数据就是海量数据的高效处理。另外，如果做一个更形象的解释，云计算相当于我们的计算机和操作系统，将大量的硬件资源虚拟化之后再进行分配使用。

就目前而言，要想发展好大数据，就离不开云计算，我们在进行大数据的时候同样也是离不开云计算的，于是很多人觉得大数据与云计算都有一定的关系，那么大家知道不知道大数据的云计算有什么关系呢？我们在这篇文章中给大家带来这个问题的答案。

首先我们说一下大数据的定义吧，大数据就是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力来适应海量、高增长率和多样化的信息资产。同样也是一种规模大到在获取、存储、管理、分析方面大大超出了传统数据库软件工具能力范围的数据集合，具有海量的数据规模、快速的数据流转、多样的数据类型和价值密度低四大特征。

那么大数据的技术有什么意义呢？大数据的意义并不是在于掌握庞大的数据信息，而在于对这些含有意义的数据进行专业化处理。换而言之，如果把大数据比作一种产业，那么这种产业实现盈利的关键，在于提高对数据的优化能力，通过优化实现数据的增值。

而大数据与云计算的关系在技术上的联系也是密不可分。大数据必然无法用单台的计算机进行处理，必须采用分布式架构。它的特色在于对海量数据进行分布式数据挖掘。但它必须依托云计算的分布式处理、分布式数据库和云存储、虚拟化技术。随着云时代的来临，大数据也吸引了越来越多的关注。分析师团队认为，大数据通常用来形容一个公司创造的大量非结构化数据和半结构化数据，这些数据在下载到关系型数据库用于分析时会花费很多的财力和物力。大数据分析常和云计算联系到一起，因为实时的大型数据集分析需要框架来向数十、数百或甚至数千的电脑分配工作。并且，大数据需要特殊的技术，以有效地处理大量的容忍经过时间内的数据。适用于大数据的技术，包括大规模并行处理数据库、数据挖掘、分布式文件系统、分布式数据库、云计算平台、互联网和可扩展的存储系统。

#### 知识图谱

知识图谱属于AI领域的是一个分支，很多人觉得它和CV（计算机视觉），ASR（语音识别），以及NLP（自然语言处理）一样都是特指的某一项技术，其实这么理解并不准确，它应该算是多种技术融合后的一种综合型技术。

知识图谱的历史最早要追溯到2012年，由google公司提出主要用于提升搜索引擎的检索效率，但随着其发展其背后更深刻意义，远不仅是提高检索效率这么简单，而是整个搜索引擎结构的整体转型：将传统基于关键字的搜索模型转向基于语义的搜索升级。

如今针对知识图谱的技术方案已被国内外多家搜索引擎公司所采用，如：美国的微软必应，中国的百度、搜狗等，都在在短短的一年内纷纷宣布了各自的“知识图谱”产品，足以看出这革新对整个搜索引擎界的整体影响。

但现在这项技术的应用并不仅拘泥于搜索引擎领域范围，很多的数据分析软件，CRM系统也开始采用基于知识图谱的模式去处理数据，从而去深入发现数据更大的价值。

知识图谱从字面上看，可以拆分为知识+图谱，这样我们就可以理解：将需要的知识数据（结构化或非结构化数据）以图谱的形式进行展示，这种简单的过程也是知识图谱的构建过程。

知识图谱为互联网上海量、异构、动态的大数据表达、组织、管理以及利用提供了一种更为有效的方式，使得网络的智能化水平更高，更加接近于人类的认知思维。

目前，知识图谱已在智能搜索、深度问答、社交网络以及一些垂直行业中有所应用，成为支撑这些应用发展的动力源泉。

#### 机器学习

机器学习（Machine Learning）是一门专门研究计算机怎样才能模拟或实现人类的学习行为，以获取新的知识或技能，使之不断改善自身的性能的学科。机器学习虽然发展了几十年，但还是存在很多没有良好解决的问题，例如图像识别、语音识别、自然语言理解、天气预测、内容推荐等等。机器主要通过大量的训练数据进行训练，程序不断地进行自我学习和修正来训练出一个模型，而模型的本质就是一堆参数用成千上万的参数来描述业务特点，从而接近人类的智力。

机器学习是一门多领域交叉学科，涉及统计学、系统辨识、逼近理论、神经网络、优化理论、计算机科学、脑科学等诸多领域。通过研究计算机怎样模拟或实现人类的学习行为， 以获取新的知识或技能。通过知识结构的不断完善与更新来提升机器自身的性能，这属于人工智能的核心领域。基于数据的机器学习是现代智能技术中的重要方法之一，研究从观测数据（样本）出发寻找规律，利用这些规律对未来数据或无法观测的数据进行预测。阿尔法Go就这项技术一个很成功的体现。

根据学习模式将机器学习分类为监督学习、无监督学习和强化学习等。根据学习方法可以将机器学习分为传统机器学习和深度学习。

深度学习是机器学习的一个子集。
深度学习的前身是人工神经网络（ANN），它的基本特点就是模仿人脑神经元传递和处理信息的模式。
有监督学习：输入的训练数据有特征、有标记，在学习中就是找到特征与标记之间的映射关系，通过标记不断纠正学习中的偏差，使预测率不断提高。这种训练数据有标记的学习称为有监督学习。
无监督学习：让计算机自己去学习怎样做一些事情，所有训练数据没有标记，只有特征。无监督学习有两种思路：第一种，训练时不为其指定明确分类但数据会呈现聚群的结构，彼此相似的类型会聚集在一起。计算机把这些没有标记的数据分成一个个组合，就是聚类；第二种，在成功时采用某种激励制度，即强化学习.

半监督学习：训练数据中有一部分有标记有一部分无标记，没有标记的数量远远大于有标记的数量（这也符合现实）。它的基本规律是：数据的分布必然不完全随机，通过结合有标记的局部特征，以及大量没标记的数据的整体分布，可以得到比较好的分类结果。

#### 自然语言处理

自然语言处理（Natural Language Processing，简称NLP）就是用计算机来处理、理解以及运用人类语言(如中文、英文等)，它属于人工智能的一个分支，是计算机科学与语言学的交叉学科，又常被称为计算语言学。由于自然语言是人类区别于其他动物的根本标志。没有语言，人类的思维也就无从谈起，所以自然语言处理体现了人工智能的最高任务与境界，也就是说，只有当计算机具备了处理自然语言的能力时，机器才算实现了真正的智能。

从研究内容来看，自然语言处理包括语法分析、语义分析、篇章理解等。从应用角度来看，自然语言处理具有广泛的应用前景。特别是在信息时代，自然语言处理的应用包罗万象，例如：机器翻译、手写体和印刷体字符识别、语音识别及文语转换、信息检索、信息抽取与过滤、文本分类与聚类、舆情分析和观点挖掘等，它涉及与语言处理相关的数据挖掘、机器学习、知识获取、知识工程、人工智能研究和与语言计算相关的语言学研究等。

值得一提的是，自然语言处理的兴起与机器翻译这一具体任务有着密切联系。机器翻译指的是利用计算机自动地将一种自然语言翻译为另外一种自然语言。由于人工进行翻译需要训练有素的双语专家，翻译工作非常耗时耗力。更不用说需要翻译一些专业领域文献时，还需要翻译者了解该领域的基本知识。世界上有超过几千种语言，而仅联合国的工作语言就有六种之多。如果能够通过机器翻译准确地进行语言间的翻译，将大大提高人类沟通和了解的效率。

自然语言处理的困难可以罗列出来很多，不过关键在于消除歧义问题，如词法分析、句法分析、语义分析等过程中存在的歧义问题，简称为消歧。而正确的消歧需要大量的知识，包括语言学知识（如词法、句法、语义、上下文等）和世界知识（与语言无关）。这带来自然语言处理的两个主要困难。首先，语言中充满了大量的歧义，这主要体现在词法、句法及语义三个层次上。歧义的产生是由于自然语言所描述的对象――人类活动非常复杂，而语言的词汇和句法规则又是有限的，这就造成同一种语言形式可能具有多种含义。另外一个方面，消除歧义所需要的知识在获取、表达以及运用上存在困难。由于语言处理的复杂性，合适的语言处理方法和模型难以设计。

目前，人们主要通过两种思路来进行自然语言处理，一种是基于规则的理性主义，另外一种是基于统计的经验主义。理性主义方法认为，人类语言主要是由语言规则来产生和描述的，因此只要能够用适当的形式将人类语言规则表示出来，就能够理解人类语言，并实现语言之间的翻译等各种自然语言处理任务。而经验主义方法则认为，从语言数据中获取语言统计知识，有效建立语言的统计模型。因此只要能够有足够多的用于统计的语言数据，就能够理解人类语言。然而，当面对现实世界充满模糊与不确定性时，这两种方法都面临着各自无法解决的问题。例如，人类语言虽然有一定的规则，但是在真实使用中往往伴随大量的噪音和不规范性。理性主义方法的一大弱点就是鲁棒性差，只要与规则稍有偏离便无法处理。而对于经验主义方法而言，又不能无限地获取语言数据进行统计学习，因此也不能够完美地理解人类语言。二十世纪八十年代以来的趋势就是，基于语言规则的理性主义方法不断受到质疑，大规模语言数据处理成为目前和未来一段时期内自然语言处理的主要研究目标。统计学习方法越来越受到重视，自然语言处理中越来越多地使用机器自动学习的方法来获取语言知识。迈进二十一世纪，我们已经进入了以互联网为主要标志的海量信息时代，这些海量信息大部分是以自然语言表示的。一方面，海量信息也为计算机学习人类语言提供了更多的“素材”，另一方面，这也为自然语言处理提供了更加宽广的应用舞台。例如，作为自然语言处理的重要应用，搜索引擎逐渐成为人们获取信息的重要工具，涌现出以百度、谷歌等为代表的搜索引擎巨头；机器翻译也从实验室走入寻常百姓家，谷歌、百度等公司都提供了基于海量网络数据的机器翻译和辅助翻译工具；基于自然语言处理的中文（输入法如搜狗、微软、谷歌等输入法）成为计算机用户的必备工具；带有语音识别的计算机和手机也正大行其道，协助用户更有效地工作学习。总之，随着互联网的普及和海量信息的涌现，自然语言处理正在人们的日常生活中扮演着越来越重要的作用。

#### 计算机视觉

计算机视觉，英文Computer Vision，简称CV。计算机视觉是一门研究如何使机器“看”的科学，更进一步的说，就是指用摄影机和电脑代替人眼对目标进行识别、跟踪和测量等。

计算机视觉的主要任务就是通过对采集的图片或视频进行处理以获得相应场景的信息。计算机视觉任务的主要类型有以下几种：

##### 物体检测

物体检测是视觉感知的第一步，也是计算机视觉的一个重要分支。物体检测的目标，就是用框去标出物体的位置，并给出物体的类别。

物体检测和图像分类不一样，检测侧重于物体的搜索，而且物体检测的目标必须要有固定的形状和轮廓。图像分类可以是任意的目标，这个目标可能是物体，也可能是一些属性或者场景。

##### 物体识别（狭义）

计算机视觉的经典问题便是判定一组图像数据中是否包含某个特定的物体，图像特征或运动状态。这一问题通常可以通过机器自动解决，但是到目前为止，还没有某个单一的方法能够广泛的对各种情况进行判定：在任意环境中识别任意物体。

现有技术能够也只能够很好地解决特定目标的识别，比如简单几何图形识别、人脸识别、印刷或手写文件识别，或者车辆识别。而且这些识别需要在特定的环境中，具有指定的光照，背景和目标姿态要求。

##### 图像分类

一张图像中是否包含某种物体，对图像进行特征描述是物体分类的主要研究内容。一般说来，物体分类算法通过手工特征或者特征学习方法对整个图像进行全局描述，然后使用分类器判断是否存在某类物体。
图像分类问题就是给输入图像分配标签的任务，这是计算机视觉的核心问题之一。这个过程往往与机器学习和深度学习不可分割。

##### 物体定位

如果说图像识别解决的是what，那么，物体定位解决的则是where的问题。利用计算视觉技术找到图像中某一目标物体在图像中的位置，即定位。

目标物体的定位对于计算机视觉在安防、自动驾驶等领域的应用有着至关重要的意义。

##### 图像分割

在图像处理过程中，有时会需要对图像进行分割来提取有价值的用于后继处理的部分，例如筛选特征点，或者分割一或多幅图片中含有特定目标的部分等。

图像分割指的是将数字图像细分为多个图像子区域（像素的集合，也被称作超像素）的过程。图像分割的目的是简化或改变图像的表示形式，使得图像更容易理解和分析。更精确地说，图像分割是对图像中的每个像素加标签的一个过程，这一过程使得具有相同标签的像素具有某种共同视觉特性。

“图像语意分割”是一个像素级别的物体识别，即每个像素点都要判断它的类别。它和检测的区别是，物体检测是一个物体级别的，他只需要一个框，去框住物体的位置，而通常分割是比检测要更难的问题。

计算机视觉是通过创建人工模型来模拟本由人类执行的视觉任务。其本质是模拟人类的感知与观察的一个过程。这个过程不止识别，而是包含了一系列的过程，并且最终是可以在人工系统中被理解和实现的。

#### 智能机器人

智能机器人之所以叫智能机器人，这是因为它有相当发达的“大脑”。在脑中起作用的是中央处理器，这种计算机跟操作它的人有直接的联系。最主要的是，这样的计算机可以进行按目的安排的动作。正因为这样，我们才说这种机器人才是真正的机器人，尽管它们的外表可能有所不同。

#### 类脑智能

类脑智能又称为类脑计算，上世纪80年代末，美国科学家Carver Mead首次提出类脑计算的概念。类脑计算这一想法摆脱了传统的计算模式，模仿人类神经系统的工作原理，渴求开发出快速、可靠、低耗的运算技术。类脑智能是人工智能的终极目标，但研究类脑智能不可能复制人的大脑。类脑智能希望通过研究人类大脑的工作机理并模拟出一个和人类一样具有思考、学习能力的机器人。类脑智能技术充分学习人脑的思维模式，从仿生角度努力寻求人工智能的突破。这一热门学科前景诱人，应用范围广阔。科学家们曾预言一个国家类脑智能的发展水平将极大程度影响该国在军事、工业等众多行业的发展，因此类脑智能技术的发展显得尤为重要与急迫。

### 人工智能的应用

人工智能应用（Applications of artificial intelligence）的泛围很广，包括：医药，诊断，金融贸易，机器人控制，法律，科学发现和玩具。许多千种人工智能应用深入于每种工业的基础。90年代和21世纪初，人工智能技术变成大系统的元素；但很少人认为这属于人工智能领域的成就。

#### 人工智能在金融领域的应用

银行用人工智能系统组织运作，金融投资和管理财产。2001年8月在模拟金融贸易竞赛中机器人战胜了人。

金融机构已长久用人工神经网络系统去发觉变化或规范外的要求，银行使用协助顾客服务系统；帮助核对帐目，发行信用卡和恢复密码等

#### 人工智能在医疗领域的应用

医学临床可用人工智能系统组织病床计划；并提供医学信息。

人工神经网络用来做临床诊断决策支持系统。用人工智能在医学方面还有下列潜在可能：计算机帮助解析医学图像。这样系统帮助扫描数据图像，从计算X光断层图发现疾病，典型应用是发现肿块。

心脏声音分析。

#### 人工智能在家居领域的应用

人工智能在智能家居场景中，一方面将进一步推动家居生活产品的智能化，包括照明系统、影音系统、能源管理系统、安防系统等，实现家居产品从感知到认知到决策的发展，另一方面在于智能家居系统的建立，搭载人工智能的多款产品都有望成为智能家居的核心，包括机器人、智能音箱、智能电视等产品，智能家居系统将逐步实现家居自我学习与控制，从而提供针对不同用户的个性化服务。

#### 人工智能在汽车领域的应用

基于AI的自动驾驶，自动驾驶车辆所需的处理能力是巨大的，而传统的计算机根本无法胜任这项任务。因为自动驾驶不是遵循一套规则或简单的算法，它涉及到深度学习等，换句话说，就涉及人工智能。越来越多的汽车制造商和初创企业都在开发人工智能应用程序，目前在自动驾驶领域有两家公司处于领先地位：谷歌和特斯拉。

基于AI的云服务，人工智能和云服务也是形影不离，相互作用的。无论如何，汽车都需要通过大量的数据来分析完成相应的任务。人工智能云服务的应用确保了数据的有效性，更充分体现了大数据的潜在价值。

基于AI的汽车保险，保险公司利用人工智能实时进行风险评估。

基于AI的汽车制造，人工智能不仅改变了汽车的功能，也改变了汽车的制造方式。新出现的是与人类并肩作战的智能机器人。例如2018年初，起亚汽车开始与现代合作，为其装配线开发可穿戴工业机器人。背心外骨骼(H-VEX)和无椅外骨骼(H-CEX)可穿戴机器人帮助保护工作人员的膝盖、背部和颈部，同时赋予他们更大的灵活性与更强的力量

基于AI的驾驶员监控，以色列初创企业eyeSight借助先进的TOF摄像机和红外传感器，检测驾驶员的行为，

#### 人工智能在零售领域的应用

人工智能在商业界中已经掀起了一阵风潮，其系统的庞大市场规模越来越受到重视。具体体现为店面优化，供应链优化，营销策略的变化，手势识别等方面

#### 人工智能在教育领域的应用

个性化学习，因材施教：分析内容，构建知识图谱，构建和优化内容模型，建立知识图谱，让用户可以更容易地、更准确地发现适合自己的内容。国外这方面的典型应用是分级阅读平台，推荐给用户适宜的阅读材料，并将阅读与教学联系在一起，文后带有小测验，并生成相关阅读数据报告，老师得以随时掌握学生阅读情况。

自动化辅导与答疑：AI除了应用于个性化学习方案的制定外，还落地在自动化辅导和答疑子领域，这也成为了教师面授外的补充。

智能测评：随着信息化建设、人工智能的发展，大数据、文字识别、语音识别、语义识别，使得规模化的自动批改和个性化反馈走向现实。如何利用人工智能减轻批改压力，实现规模化又个性化的作业反馈，是未来教育的重要攻克点

模拟和游戏化教学平台：寓教于乐也是现代教育理念之一。

教育决策：AI 能做的远远不止大学专业选择的分析决策，AI 帮助决策将越来越多地影响我们生活的方方面面

幼儿早教机器人：早教的未来在移动和智能。儿童机器人的门槛不在技术这块，而在于内容、交互方式。

## 物联网概述

### 物联网的发展

物联网的概念英文术语为**Internet of Things**。物联网是指按照约定的协议，将具有**感知、通信、计算**功能的智能物体、系统、信息资源互联起来，实现对物理世界**泛在感知、可靠传输、智慧处理**的智能服务系统。20世纪90年代有关物联网的研究开始萌芽，此后其概念不断地演进和发展。1999年美国麻省理工学院的Kevin Ashton和他的同事首次提出Internet of Things的概念。同年，中科院启动了传感网的研究，并取得了一些科研成果，建立了一些适用的传感网。2005年，国际电信联盟（ITU）在《The Internet of Things》报告中对物联网概念进行扩展。无所不在的物联网通信时代即将来临。

![1](https://md.hass.live/niji/2019-08-20-7762387-0e9b38a90f7a7cf5.png)

进入2009年，物联网得到了真正的起步。

**2019年1月**，奥巴马就任美国总统后，与工商界代表举行了一次圆桌会议。IBM公司代表提出了**智慧地球**的研究计划，建议新政府投资新一代的智慧型基础设施。当年，美国将新能源和物联网列为振兴经济的两大核心武器；

**2019年8月**，温家宝总理在视察无锡时提出建设“感知中国”中心，物联网被正式列为国家五大新兴战略性产业之一，写入“政府工作报告”，物联网在中国受到了全社会极大的关注；

**2019年9月**，欧盟第七框架下RFID和物联网研究项目簇（European Research Cluster on the Internet of Things）发布了《物联网战略研究路线图》研究报告，定义为基于标准的和可互操作的通信协议且具有自配置能力的动态的全球网络基础架构。
至此，物联网真正的建立了起来，随后的几年物联网开始迈上了新的台阶。我们要学习物联网，必然要从技术层面入手，当下从技术的角度来理解，物联网的发展共分为4个阶段，具体如下图所示：

![2](https://md.hass.live/niji/2019-08-20-7762387-59643b998aa8a6c1.png)

物联网当今在中国的受关注程度是在美国、欧盟及其他各国不可比拟的，中国在物联网理念和应用方面可以说是已经走在了世界的前面。近年来，电子信息技术和人工智能应用的发展促使物联网的内涵和外延有了很大的拓展，物联网已经表现为信息技术和通信技术的发展融合，是信息社会发展的趋势。

### 物联网的关键技术

物联网的关键技术主要包括三个方面：各类终端实现“全面感知”；电信网、互联网等融合实现“可靠创术”；云计算等技术对海量数据的“智能处理”。通过这三个方面的有机结合实现各类资源的“虚拟”和“共享”。

#### 全面感知

全面感知是指利用射频识别（RFID）、传感器、定位器和二维码等手段随时随地对物体进行信息采集和获取。

![3](https://md.hass.live/niji/2019-08-20-7762387-ae563cd9debf05c9.png)

#### 可靠传输

可靠传输是指通过各种电信网络与互联网的融合，对接收到的感知信息进行实时远程传送，实现信息的交互和共享，并进行各种有效的处理。在这一过程中，通常需要用到现有的电信运行网络，包括无线和有线网络。由于传感器网络是一个局部的无线网，因此无线移动通信网、4G、5G网络是作为承载物联网的一个有力的支撑。

![4](https://md.hass.live/niji/2019-08-20-7762387-4a60663d32522c1c.png)

#### 智能处理

物联网是一个智能的网络，面对采集的海量数据，必须通过智能分析和处理才能实现智能化。智能处理是指利用云计算、数据挖掘、模糊识别等各种智能计算技术，对随时接收到的跨地域、跨行业、跨部门的海量数据和信息进行分析处理，提升对物理世界、济社会各种活动和变化的洞察力，实现智能化的决策和控制。

![5](https://md.hass.live/niji/2019-08-20-7762387-486ed4cf97222928.png)

### 边缘计算

### 物联网的应用

#### 物联网在家居领域的应用

#### 物联网在教育领域的应用

## 开源硬件概述

### 开源硬件的发展

### 流行的开源硬件

#### NVIDIA Jetson Nano

#### 树莓派

#### Arduino

#### ESPRESSIF

esp8266是一个价格低廉的开发板，包含WiFi模块和GPIO，可以连接传感器、舵机、马达等各种设备。使用Arduino IDE进行开发编程。可通过网络、串口和蓝牙等多种方式进行通信。

### 传感器
